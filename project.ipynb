{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b96739e5-a33e-4768-a618-12d9aa14f634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-20 16:25:13.440908: I tensorflow/core/platform/cpu_feature_guard.cc:181] Beginning TensorFlow 2.15, this package will be updated to install stock TensorFlow 2.15 alongside Intel's TensorFlow CPU extension plugin, which provides all the optimizations available in the package and more. If a compatible version of stock TensorFlow is present, only the extension will get installed. No changes to code or installation setup is needed as a result of this change.\n",
      "More information on Intel's optimizations for TensorFlow, delivered as TensorFlow extension plugin can be viewed at https://github.com/intel/intel-extension-for-tensorflow.\n",
      "[nltk_data] Downloading package punkt to /home/bowenyi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/bowenyi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/bowenyi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/bowenyi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: prettytable in /home/bowenyi/.local/lib/python3.11/site-packages (3.9.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda/lib/python3.11/site-packages (from prettytable) (0.2.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in /home/bowenyi/.local/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/anaconda/lib/python3.11/site-packages (from sentence-transformers) (4.35.2)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda/lib/python3.11/site-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/anaconda/lib/python3.11/site-packages (from sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: torchvision in /home/bowenyi/.local/lib/python3.11/site-packages (from sentence-transformers) (0.16.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda/lib/python3.11/site-packages (from sentence-transformers) (1.26.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda/lib/python3.11/site-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda/lib/python3.11/site-packages (from sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: nltk in /opt/anaconda/lib/python3.11/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /home/bowenyi/.local/lib/python3.11/site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/anaconda/lib/python3.11/site-packages (from sentence-transformers) (0.19.4)\n",
      "Requirement already satisfied: filelock in /opt/anaconda/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/bowenyi/.local/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: sympy in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/anaconda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence-transformers) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/anaconda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/anaconda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.2)\n",
      "Requirement already satisfied: click in /opt/anaconda/lib/python3.11/site-packages (from nltk->sentence-transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda/lib/python3.11/site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda/lib/python3.11/site-packages (from torchvision->sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda/lib/python3.11/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.corpus import wordnet\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords.add('u')\n",
    "stopwords.add('th')\n",
    "sys.path.append(\"/home/bowenyi/.local/lib/python3.11/site-packages\")\n",
    "!pip install prettytable\n",
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2156140-9d81-4823-80de-3d9ef3b5ddf2",
   "metadata": {},
   "source": [
    "### **1. Preprocess dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd94693-bad7-49d7-91b4-9dc1aef9abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before = pd.read_csv(\"/shared/3/projects/benlitterer/podcastData/processed/beforeFloydMonth/beforeFMonth.tsv\", lineterminator = '\\n', low_memory=False)\n",
    "df_in = pd.read_csv(\"/shared/3/projects/benlitterer/podcastData/processed/floydMonth/floydMonthEnSHORT.csv\", lineterminator = '\\n', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f37e155-b62b-453b-91d3-1a79c99634db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before = df_before.dropna(subset=['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10'], how='all')\n",
    "df_before = df_before.drop_duplicates()\n",
    "df_before = df_before.drop_duplicates(subset=['potentialOutPath'])\n",
    "\n",
    "df_in = df_in.dropna(subset=['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10'], how='all')\n",
    "df_in = df_in.drop_duplicates()\n",
    "df_in = df_in.drop_duplicates(subset=['potentialOutPath'])\n",
    "\n",
    "df_before['potentialOutPath'] = df_before['potentialOutPath'].apply(lambda x: \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/beforeFMonth\" + x)\n",
    "df_in['potentialOutPath'] = df_in['potentialOutPath'].apply(lambda x: \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/floydMonth\" + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b1dda6-060e-41d3-bf4a-420ace9d0c8c",
   "metadata": {},
   "source": [
    "#### **1.1 Introduce an is_news column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2000e87b-b775-4c03-906a-9fb96c7f08bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "df_before['is_news'] = df_before[['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10']].apply(lambda x: 1 if 'news' in x.values else 0, axis=1)\n",
    "df_in['is_news'] = df_in[['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10']].apply(lambda x: 1 if 'news' in x.values else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142d1c6d-39a5-40ff-8882-6d5058e92c2d",
   "metadata": {},
   "source": [
    "#### **1.2 Downsample the training data to 4:1 distribution**\n",
    "- Training data are df_before_news and df_after_news (unavailable yet)\n",
    "- non-news : news = 4 : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24e8115d-83ac-46c7-bf0d-acef93fcd932",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before_news = df_before[df_before['is_news'] == 1]\n",
    "n_before_news = df_before_news.shape[0]\n",
    "df_before_no_news = df_before[df_before['is_news'] == 0].sample(n=n_before_news*4, replace=False, random_state=387)\n",
    "df_before_news = pd.concat([df_before_news, df_before_no_news], ignore_index=True)\n",
    "df_before_news = df_before_news.sample(frac=1, random_state=387).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec8f5e-44b2-446e-8ddd-7f4ce1be50d6",
   "metadata": {},
   "source": [
    "### **2. Obtain the training and non-training data**\n",
    "- df_train: dataframe for training data\n",
    "- df_non_train: dataframe for non-training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "152bc70c-ee7e-4163-a3a1-a807522f1722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"\\n\", \"\", text)  # remove line breaks\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # remove [Music], (Audio), etc.\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    text = text.lower()    # convert to lowercase\n",
    "    text = re.sub(r'\\b\\w+\\.com\\b', '', text)  # remove something.com\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)   # remove hyperlinks\n",
    "    text = re.sub(r\"\\d+\", \"\", text)   # remove digits and currencies \n",
    "    text = re.sub(r'[\\$\\d+\\d+\\$]', \"\", text)\n",
    "    text = re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', text)   # remove dates\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r' ', text)  # remove non-ascii\n",
    "    text = contractions.fix(text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)   # remove punctuation\n",
    "\n",
    "    filtered_tokens = [word for word in word_tokenize(text) if not word in stopwords]\n",
    "    pos_tags = pos_tag(filtered_tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags if lemmatizer.lemmatize(word, get_wordnet_pos(tag)) not in stopwords]\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "\n",
    "    return lemmatized_text\n",
    "\n",
    "\n",
    "def split_transcript(transcript):\n",
    "    transcript['content'] = transcript['content'].fillna('').astype(str)\n",
    "    start_time = 0\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    end_of_sentence = ['.', '!', '?', ']', ')']\n",
    "\n",
    "    for index, row in transcript.iterrows():\n",
    "        content = str(row['content'])\n",
    "        if content.strip() != '':\n",
    "            if row['end'] - start_time < 60:\n",
    "                chunk += content\n",
    "            else:\n",
    "                chunk += content\n",
    "                if any(ele in content for ele in end_of_sentence):\n",
    "                    chunk = preprocess_text(chunk)\n",
    "                    if chunk.strip() != '':\n",
    "                        chunks.append(chunk)\n",
    "                        start_time = row['end']\n",
    "                    chunk = \"\"\n",
    "\n",
    "    if len(chunk) != 0:\n",
    "        chunk = preprocess_text(chunk)\n",
    "        if chunk.strip() != '':\n",
    "            chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fc46b10-b2e9-46cd-bee7-18684b0cbb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_transcript_miniLM(transcript):\n",
    "    transcript['content'] = transcript['content'].fillna('').astype(str)\n",
    "    text = ''\n",
    "    for ind, row in transcript.iterrows():\n",
    "        text += str(row['content'])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "245c9d03-9539-4275-ab96-702e45722929",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.DataFrame(columns=['raw_X', 'chunked_X', 'y', 'real_y', 'prob', 'path'])\n",
    "for index, row in df_before_news.iterrows():\n",
    "    path = row[\"potentialOutPath\"]\n",
    "    if os.path.isfile(path):\n",
    "        transcript = pd.read_csv(path, usecols=['start', 'end', 'content'])\n",
    "        raw_X = split_transcript_miniLM(transcript)\n",
    "        chunked_X = split_transcript(transcript)\n",
    "        y = row['is_news']\n",
    "        real_y = -1\n",
    "        prob = -1\n",
    "        df_news.loc[index] = [raw_X, chunked_X, y, real_y, prob, path]\n",
    "\n",
    "df_news = df_news.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbd1659e-5156-4965-b6ea-e555321c0e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = df_news[df_news['chunked_X'].apply(lambda x: len(x) > 0)]\n",
    "df_train, df_non_train = train_test_split(df_news, test_size=0.5, random_state=1)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_non_train = df_non_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd6d4c-05ba-456a-bf48-e353337dfc43",
   "metadata": {},
   "source": [
    "### **3. Train two models**\n",
    "- MiniLM\n",
    "- Logistic regression (l2-loss, min_df = 1000 or so)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31357983-1544-498f-ad15-aa6442005c88",
   "metadata": {},
   "source": [
    "#### **3.1 MiniLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c0821e2-3430-48ff-8502-a06b206ff971",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_miniLM = pd.DataFrame(columns=['label', 'text'])\n",
    "for index, row in df_train.iterrows():\n",
    "    df_train_miniLM.loc[index] = [row['y'], row['raw_X']]\n",
    "\n",
    "ds_train_miniLM = Dataset.from_pandas(df_train_miniLM)\n",
    "ds_train_miniLM = ds_train_miniLM.remove_columns([\"__index_level_0__\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7407e111-1663-4f16-8b07-9821b0b1ed3d",
   "metadata": {},
   "source": [
    "### **Note: Once memory is more free, change per_device_train_batch_size and epoch as per David's advice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105a9dd7-cd00-4c12-b8e6-ebfd527e8375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9444cd7c9c364be8b3b5621af0a6c315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39694 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='933' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  8/933 31:15 < 80:17:53, 0.00 it/s, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/MiniLM-L12-H384-uncased\")\n",
    "miniLM = AutoModelForSequenceClassification.from_pretrained(\"microsoft/MiniLM-L12-H384-uncased\", num_labels=2)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "tokenized_datasets = ds_train_miniLM.map(tokenize_function, batched=True)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", per_device_train_batch_size=128, num_train_epochs=3)\n",
    "trainer = Trainer(\n",
    "    model=miniLM,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a4d891-fcb0-4ef0-98ca-f37c0f45e466",
   "metadata": {},
   "source": [
    "#### **3.2 Logistic regression**\n",
    "- Trained on unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a905e4b-ad43-4ebe-9433-536fb48490ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = [], []\n",
    "\n",
    "for index, row in df_train.iterrows():\n",
    "    for chunk in row['chunked_X']:\n",
    "        X_train.append(chunk)\n",
    "    y_train.extend([row['y']] * len(row['chunked_X']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c893c864-070d-4669-a22b-33ddcb2b6378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), min_df=0.025)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "log_reg = LogisticRegression(random_state=1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9af179-aa07-4b48-9d75-a3acde3084f9",
   "metadata": {},
   "source": [
    "### **4. Model calibration on non-train data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cddbe9-63d2-45a0-9482-6967d1e23895",
   "metadata": {},
   "source": [
    "#### **4.1 Mini-LM**\n",
    "- Predict probabilities on each row\n",
    "- Build strata profile\n",
    "- Annotate 10 random samples from each stratum\n",
    "- Calculate F-1 for each cutoff (9 in total)\n",
    "- Find the best cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f027a-bdd1-4a6c-9377-10f3e06f5f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_train.rename(columns={'prob':'prob_miniLM'}, inplace=True)\n",
    "df_non_train['prob_log_reg'] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc426ad-955a-47b2-900e-4fb4d552f55a",
   "metadata": {},
   "source": [
    "#### **4.1.1 Predict probabilities on each row & fill in the probability score column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91ce373-5aba-4a6e-9e58-37d5a470bb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "prediction_inputs = df_non_train['raw_X'].tolist()\n",
    "predict_dset = Dataset.from_pandas(pd.DataFrame({\"text\": prediction_texts}))\n",
    "predict_dset = predict_dset.remove_columns([\"__index_level_0__\"])\n",
    "\n",
    "tokenized_predict_dset = predict_dset.map(tokenize_function, batched=True)\n",
    "predictions = trainer.predict(tokenized_predict_dset)\n",
    "\n",
    "# Apply softmax to the model's raw predictions to get probabilities\n",
    "probs = softmax(predictions.predictions, axis=1)[:, 1]\n",
    "\n",
    "df_non_train['prob_miniLM'] = probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c475b4-6b8c-4abe-aa16-e57e43988079",
   "metadata": {},
   "source": [
    "#### **4.1.2 Build strata profile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7500d2ef-aeb9-457d-85c0-f40046db3e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18c3179d-1cef-480b-8695-a03a9911b56f",
   "metadata": {},
   "source": [
    "#### **4.2 Logistic Regression**\n",
    "- Predict probabilities on each row, store output probabilities into \"prob_log_reg\"\n",
    "- Build strata profile\n",
    "- Annotate 10 random samples from each stratum\n",
    "- Calculate F-1 for each cutoff (9 in total)\n",
    "- Find the best cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701dbc8f-84d8-471f-ba61-f68412526995",
   "metadata": {},
   "source": [
    "#### **4.1.1 Predict probabilities on each row & fill in the probability score column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b1eba4-4391-4579-86db-5309e79d99d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.transform(df_non_train['text'])\n",
    "y = log_reg.predict_proba(X)\n",
    "df_non_train['prob_log_reg'] = probs[:, 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
