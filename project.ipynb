{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b96739e5-a33e-4768-a618-12d9aa14f634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bowenyi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/bowenyi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/bowenyi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/bowenyi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: prettytable in /home/bowenyi/.local/lib/python3.11/site-packages (3.9.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda/lib/python3.11/site-packages (from prettytable) (0.2.5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.corpus import wordnet\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords.add('u')\n",
    "stopwords.add('th')\n",
    "sys.path.append(\"/home/bowenyi/.local/lib/python3.11/site-packages\")\n",
    "!pip install prettytable\n",
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2156140-9d81-4823-80de-3d9ef3b5ddf2",
   "metadata": {},
   "source": [
    "### **1. Preprocess dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd94693-bad7-49d7-91b4-9dc1aef9abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before = pd.read_csv(\"/shared/3/projects/benlitterer/podcastData/processed/beforeFloydMonth/beforeFMonth.tsv\", lineterminator = '\\n', low_memory=False)\n",
    "df_in = pd.read_csv(\"/shared/3/projects/benlitterer/podcastData/processed/floydMonth/floydMonthEnSHORT.csv\", lineterminator = '\\n', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f37e155-b62b-453b-91d3-1a79c99634db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before = df_before.dropna(subset=['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10'], how='all')\n",
    "df_before = df_before.drop_duplicates()\n",
    "df_before = df_before.drop_duplicates(subset=['potentialOutPath'])\n",
    "\n",
    "df_in = df_in.dropna(subset=['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10'], how='all')\n",
    "df_in = df_in.drop_duplicates()\n",
    "df_in = df_in.drop_duplicates(subset=['potentialOutPath'])\n",
    "\n",
    "df_before['potentialOutPath'] = df_before['potentialOutPath'].apply(lambda x: \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/beforeFMonth\" + x)\n",
    "df_in['potentialOutPath'] = df_in['potentialOutPath'].apply(lambda x: \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/floydMonth\" + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b1dda6-060e-41d3-bf4a-420ace9d0c8c",
   "metadata": {},
   "source": [
    "#### **1.1 Introduce an is_news column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2000e87b-b775-4c03-906a-9fb96c7f08bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "df_before['is_news'] = df_before[['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10']].apply(lambda x: 1 if 'news' in x.values else 0, axis=1)\n",
    "df_in['is_news'] = df_in[['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10']].apply(lambda x: 1 if 'news' in x.values else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142d1c6d-39a5-40ff-8882-6d5058e92c2d",
   "metadata": {},
   "source": [
    "#### **1.2 Downsample the training data to 4:1 distribution**\n",
    "- Training data are df_before_news and df_after_news (unavailable yet)\n",
    "- non-news : news = 4 : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24e8115d-83ac-46c7-bf0d-acef93fcd932",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before_news = df_before[df_before['is_news'] == 1]\n",
    "n_before_news = df_before_news.shape[0]\n",
    "df_before_no_news = df_before[df_before['is_news'] == 0].sample(n=n_before_news*4, replace=False, random_state=387)\n",
    "df_before_news = pd.concat([df_before_news, df_before_no_news], ignore_index=True)\n",
    "df_before_news = df_before_news.sample(frac=1, random_state=387).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec8f5e-44b2-446e-8ddd-7f4ce1be50d6",
   "metadata": {},
   "source": [
    "### **2. Obtain the training and non-training data**\n",
    "- df_train: dataframe for training data\n",
    "- df_non_train: dataframe for non-training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "152bc70c-ee7e-4163-a3a1-a807522f1722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"\\n\", \"\", text)  # remove line breaks\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # remove [Music], (Audio), etc.\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    text = text.lower()    # convert to lowercase\n",
    "    text = re.sub(r'\\b\\w+\\.com\\b', '', text)  # remove something.com\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)   # remove hyperlinks\n",
    "    text = re.sub(r\"\\d+\", \"\", text)   # remove digits and currencies \n",
    "    text = re.sub(r'[\\$\\d+\\d+\\$]', \"\", text)\n",
    "    text = re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', text)   # remove dates\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r' ', text)  # remove non-ascii\n",
    "    text = contractions.fix(text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)   # remove punctuation\n",
    "\n",
    "    filtered_tokens = [word for word in word_tokenize(text) if not word in stopwords]\n",
    "    pos_tags = pos_tag(filtered_tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags if lemmatizer.lemmatize(word, get_wordnet_pos(tag)) not in stopwords]\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "\n",
    "    return lemmatized_text\n",
    "\n",
    "\n",
    "def split_transcript(transcript):\n",
    "    transcript['content'] = transcript['content'].fillna('').astype(str)\n",
    "    start_time = 0\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    end_of_sentence = ['.', '!', '?', ']', ')']\n",
    "\n",
    "    for index, row in transcript.iterrows():\n",
    "        content = str(row['content'])\n",
    "        if content.strip() != '':\n",
    "            if row['end'] - start_time < 60:\n",
    "                chunk += content\n",
    "            else:\n",
    "                chunk += content\n",
    "                if any(ele in content for ele in end_of_sentence):\n",
    "                    chunk = preprocess_text(chunk)\n",
    "                    if chunk.strip() != '':\n",
    "                        chunks.append(chunk)\n",
    "                        start_time = row['end']\n",
    "                    chunk = \"\"\n",
    "\n",
    "    if len(chunk) != 0:\n",
    "        chunk = preprocess_text(chunk)\n",
    "        if chunk.strip() != '':\n",
    "            chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0fc46b10-b2e9-46cd-bee7-18684b0cbb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_transcript_miniLM(transcript):\n",
    "    transcript['content'] = transcript['content'].fillna('').astype(str)\n",
    "    text = ''\n",
    "    for ind, row in transcript.iterrows():\n",
    "        text += str(row['content'])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245c9d03-9539-4275-ab96-702e45722929",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.DataFrame(columns=['raw_X', 'chunked_X', 'y', 'real_y', 'prob', 'path'])\n",
    "for index, row in df_before_news.iterrows():\n",
    "    path = row[\"potentialOutPath\"]\n",
    "    if os.path.isfile(path):\n",
    "        transcript = pd.read_csv(path, usecols=['start', 'end', 'content'])\n",
    "        raw_X = split_transcript_miniLM(transcript)\n",
    "        chunked_X = split_transcript(transcript)\n",
    "        y = row['is_news']\n",
    "        real_y = -1\n",
    "        prob = -1\n",
    "        df_news.loc[index] = [raw_X, chunked_X, y, real_y, prob, path]\n",
    "\n",
    "df_news = df_news.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "bbd1659e-5156-4965-b6ea-e555321c0e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = df_news[df_news['chunked_X'].apply(lambda x: len(x) > 0)]\n",
    "df_train, df_non_train = train_test_split(df_news, test_size=0.5, random_state=1)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_non_train = df_non_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd6d4c-05ba-456a-bf48-e353337dfc43",
   "metadata": {},
   "source": [
    "### **3. Train two models**\n",
    "- MiniLM\n",
    "- Logistic regression (reset min_df to 1000 or so)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31357983-1544-498f-ad15-aa6442005c88",
   "metadata": {},
   "source": [
    "#### **3.1 MiniLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "5c0821e2-3430-48ff-8502-a06b206ff971",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_miniLM = pd.DataFrame(columns=['label', 'text'])\n",
    "for index, row in df_train.iterrows():\n",
    "    df_train_miniLM.loc[index] = [row['y'], row['raw_X']]\n",
    "\n",
    "ds_train_miniLM = Dataset.from_pandas(df_train_miniLM)\n",
    "ds_train_miniLM = ds_train_miniLM.remove_columns([\"__index_level_0__\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "105a9dd7-cd00-4c12-b8e6-ebfd527e8375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/MiniLM-L12-H384-uncased\")\n",
    "miniLM = AutoModelForSequenceClassification.from_pretrained(\"microsoft/MiniLM-L12-H384-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e6a0754e-0220-4e63-8f6d-bc6e990c02be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length : 729.5177860633848\n"
     ]
    }
   ],
   "source": [
    "average_length = df_train_miniLM['text'].apply(len).mean()\n",
    "print(\"Average length :\", average_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "9b4a2e7e-07e5-4053-8224-f00e917799b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4119f41b4a9048e39ea1bf17721fd15e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39694 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "tokenized_datasets = ds_train_miniLM.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b757e-ad17-469e-8958-70dabded7964",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"test_trainer\", per_device_train_batch_size=32, num_train_epochs=10)\n",
    "trainer = Trainer(\n",
    "    model=miniLM,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a4d891-fcb0-4ef0-98ca-f37c0f45e466",
   "metadata": {},
   "source": [
    "#### **3.2 Logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0a905e4b-ad43-4ebe-9433-536fb48490ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1589725322.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[195], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = [], []\n",
    "\n",
    "for index, row in df_train.iterrows():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c893c864-070d-4669-a22b-33ddcb2b6378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
