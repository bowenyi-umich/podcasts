{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af4fefd9-9ec1-439c-b779-b2ca3f1fad1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 23:35:35.110718: I tensorflow/core/platform/cpu_feature_guard.cc:181] Beginning TensorFlow 2.15, this package will be updated to install stock TensorFlow 2.15 alongside Intel's TensorFlow CPU extension plugin, which provides all the optimizations available in the package and more. If a compatible version of stock TensorFlow is present, only the extension will get installed. No changes to code or installation setup is needed as a result of this change.\n",
      "More information on Intel's optimizations for TensorFlow, delivered as TensorFlow extension plugin can be viewed at https://github.com/intel/intel-extension-for-tensorflow.\n",
      "[nltk_data] Downloading package punkt to /home/bowenyi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/bowenyi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/bowenyi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/bowenyi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: prettytable in /home/bowenyi/.local/lib/python3.11/site-packages (3.9.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda/lib/python3.11/site-packages (from prettytable) (0.2.5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords.add('u')\n",
    "stopwords.add('th')\n",
    "sys.path.append(\"/home/bowenyi/.local/lib/python3.11/site-packages\")\n",
    "!pip install prettytable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e363da0-5e83-46eb-b2dd-c7bcecc6a144",
   "metadata": {},
   "source": [
    "## 1. Preprocess dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfc135ad-68ef-44ac-875f-3db7642afc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before = pd.read_csv(\"/shared/3/projects/benlitterer/podcastData/processed/beforeFloydMonth/beforeFMonth.tsv\", lineterminator = '\\n', low_memory=False)\n",
    "df_in = pd.read_csv(\"/shared/3/projects/benlitterer/podcastData/processed/floydMonth/floydMonthEnSHORT.csv\", lineterminator = '\\n', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "009126e8-331d-4c8b-8bef-042525761355",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before = df_before.dropna(subset=['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10'], how='all')\n",
    "df_before = df_before.drop_duplicates()\n",
    "df_before = df_before.drop_duplicates(subset=['potentialOutPath'])\n",
    "\n",
    "df_in = df_in.dropna(subset=['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10'], how='all')\n",
    "df_in = df_in.drop_duplicates()\n",
    "df_in = df_in.drop_duplicates(subset=['potentialOutPath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61c8f5fe-827f-4a35-a804-6d5eedf82679",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before['potentialOutPath'] = df_before['potentialOutPath'].apply(lambda x: \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/beforeFMonth\" + x)\n",
    "df_in['potentialOutPath'] = df_in['potentialOutPath'].apply(lambda x: \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/floydMonth\" + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9ac927-3107-4591-9dca-a4a83c38132c",
   "metadata": {},
   "source": [
    "### *1.3 Introduce two columns that represent data labels: is_news and is_politics:* \n",
    "\n",
    "In Ashwin's 2021 paper Political Discussion is Abundant..., a podcast is political if it is about political institution (definition modified). Thus, if a podcast is categorized as \"government\", it is political."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f3a67ed-2d0a-4075-80a6-ce193614483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "df_before['is_news'] = df_before[['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10']].apply(lambda x: 1 if 'news' in x.values else 0, axis=1)\n",
    "df_before['is_politics'] = df_before[['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10']].apply(lambda x: 1 if ('politics' in x.values or 'government' in x.values) else 0, axis=1)\n",
    "\n",
    "df_in['is_news'] = df_in[['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10']].apply(lambda x: 1 if 'news' in x.values else 0, axis=1)\n",
    "df_in['is_politics'] = df_in[['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10']].apply(lambda x: 1 if ('politics' in x.values or 'government' in x.values) else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ada2f4-e8d9-4545-b4b6-b13618180351",
   "metadata": {},
   "source": [
    "### *1.4 Introduce three new columns:*\n",
    "- \"transcribed\": if a transcript has been transcribed\n",
    "- \"ground_truth_pol\": ground truth data on is_political: 0 or 1. Default 2. \n",
    "- \"grount_truth_news\": ground truth data on is_news: 0 or 1. Default 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b337633c-2ae6-4d30-a810-891e19660002",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before['transcribed'] = 0\n",
    "df_before['ground_truth_pol'] = 2\n",
    "df_before['ground_truth_news'] = 2\n",
    "\n",
    "df_in['transcribed'] = 0\n",
    "df_in['ground_truth_pol'] = 2\n",
    "df_in['ground_truth_news'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a0e323-d787-49bb-91ad-776312f9ec05",
   "metadata": {},
   "source": [
    "### *1.5 Downsample non-news/non-political podcasts to 4:1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9b87066-f709-4e4d-be42-fcf57c69ecc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_before' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Before Floyd Month:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# is_politics:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# df_before_pol = df_before[df_before['is_politics'] == 1].copy()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# is_news:\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m df_before_news \u001b[38;5;241m=\u001b[39m df_before[df_before[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_news\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     11\u001b[0m n_before_news \u001b[38;5;241m=\u001b[39m df_before_news\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     12\u001b[0m df_before_no_news \u001b[38;5;241m=\u001b[39m df_before[df_before[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_news\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msample(n\u001b[38;5;241m=\u001b[39mn_before_news \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m387\u001b[39m)\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_before' is not defined"
     ]
    }
   ],
   "source": [
    "# Before Floyd Month:\n",
    "# is_politics:\n",
    "# df_before_pol = df_before[df_before['is_politics'] == 1].copy()\n",
    "# n_before_pol = df_before_pol.shape[0]\n",
    "# df_before_no_pol = df_before[df_before['is_politics'] == 0].sample(n=n_before_pol * 4, replace=False, random_state=387).copy()\n",
    "# df_before_pol = pd.concat([df_before_pol, df_before_no_pol], ignore_index=True)\n",
    "# df_before_pol = df_before_pol.sample(frac=1, random_state=387).reset_index(drop=True)\n",
    "\n",
    "# is_news:\n",
    "df_before_news = df_before[df_before['is_news'] == 1].copy()\n",
    "n_before_news = df_before_news.shape[0]\n",
    "df_before_no_news = df_before[df_before['is_news'] == 0].sample(n=n_before_news * 4, replace=False, random_state=387).copy()\n",
    "df_before_news = pd.concat([df_before_news, df_before_no_news], ignore_index=True)\n",
    "df_before_news = df_before_news.sample(frac=1, random_state=387).reset_index(drop=True)\n",
    "\n",
    "# In Floyd Month:\n",
    "# is_politics:\n",
    "# df_in_pol = df_in[df_in['is_politics'] == 1].copy()\n",
    "# n_in_pol = df_in_pol.shape[0]\n",
    "# df_in_no_pol = df_in[df_in['is_politics'] == 0].sample(n=n_in_pol * 4, replace=False, random_state=387).copy()\n",
    "# df_in_pol = pd.concat([df_in_pol, df_in_no_pol], ignore_index=True).sample(frac=1, random_state=387).reset_index(drop=True)\n",
    "\n",
    "# is_news:\n",
    "df_in_news = df_in[df_in['is_news'] == 1].copy()\n",
    "n_in_news = df_in_news.shape[0]\n",
    "df_in_no_news = df_in[df_in['is_news'] == 0].sample(n=n_in_news * 4, replace=False, random_state=387).copy()\n",
    "df_in_news = pd.concat([df_in_news, df_in_no_news], ignore_index=True).sample(frac=1, random_state=387).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8c989f-b84e-42fc-af01-937d7b5ad9d7",
   "metadata": {},
   "source": [
    "##### Sidenote: \n",
    "Pipeline for cross-validation (if used)\n",
    "1. Split the dataset (non-inFMonth) into 85% for cross-validation (cv) and 15% for test. \n",
    "2. Intiate three models (Logistic Regression, Multinomial Naive Bayes, SVM) and a range parameters (RandomSearch)\n",
    "3. Pick the model and their parameters with the best performance on cv  \n",
    "3. Train the model on the entire 85% dataset that we use for cv\n",
    "4. Evaluate the model performance on two test sets: test_cv (15% of the dataset) and test_inFMonth (inFMonth data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583e9cb2-305c-460a-9c16-45d8f5b64541",
   "metadata": {},
   "source": [
    "### *1.5 Functions that chunkify transcripts and preprocess the chunks:*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7485631d-653a-418d-9a56-62337cc7f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"\\n\", \"\", text)  # remove line breaks\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # remove [Music], (Audio), etc.\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    text = text.lower()    # convert to lowercase\n",
    "    text = re.sub(r'\\b\\w+\\.com\\b', '', text)  # remove something.com\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)   # remove hyperlinks\n",
    "    text = re.sub(r\"\\d+\", \"\", text)   # remove digits and currencies \n",
    "    text = re.sub(r'[\\$\\d+\\d+\\$]', \"\", text)\n",
    "    text = re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', text)   # remove dates\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r' ', text)  # remove non-ascii\n",
    "    text = contractions.fix(text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)   # remove punctuation\n",
    "\n",
    "    filtered_tokens = [word for word in word_tokenize(text) if not word in stopwords]\n",
    "    pos_tags = pos_tag(filtered_tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags if lemmatizer.lemmatize(word, get_wordnet_pos(tag)) not in stopwords]\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "\n",
    "    return lemmatized_text\n",
    "\n",
    "# Split transcripts for non-bert models\n",
    "def split_transcript(transcript):\n",
    "    transcript['content'] = transcript['content'].fillna('').astype(str)\n",
    "    start_time = 0\n",
    "    chunks = []  # List of lists. Each sub-list is a string of chunked text\n",
    "    chunk = \"\"\n",
    "    end_of_sentence = ['.', '!', '?', ']', ')']\n",
    "\n",
    "    for index, row in transcript.iterrows():\n",
    "        content = str(row['content'])\n",
    "        if content.strip() != '':\n",
    "            if row['end'] - start_time < 60:\n",
    "                chunk += content\n",
    "            else:\n",
    "                chunk += content\n",
    "                if any(ele in content for ele in end_of_sentence):  # if we reach the end of a sentence\n",
    "                    chunk = preprocess_text(chunk)\n",
    "                    if chunk.strip() != '':  # ignore chunks that are only white spaces or empty\n",
    "                        list = []\n",
    "                        list.append(chunk)\n",
    "                        chunks.append(list)\n",
    "                        start_time = row['end']\n",
    "                    chunk = \"\"\n",
    "\n",
    "    if len(chunk) != 0:\n",
    "        chunk = preprocess_text(chunk)\n",
    "        if chunk.strip() != '':\n",
    "            list = []\n",
    "            list.append(chunk)\n",
    "            chunks.append(list)\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc88fae3-ead3-4fe0-970e-ccd2dde88e2b",
   "metadata": {},
   "source": [
    "### *1.6 Obtain beforeFMonth Data and split it into train-dev-test sets*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f800b4-ab28-4741-8e2a-ac82ceb805ca",
   "metadata": {},
   "source": [
    "#### *1.6.1 Non-Bert models: List of features and labels*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b1eeaf-c20f-422d-997e-8d68f3965ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_before_pol = []\n",
    "y_before_pol = []\n",
    "x_before_news = []\n",
    "y_before_news = []\n",
    "\n",
    "# is_pol:\n",
    "for index, row in df_before_pol.iterrows():\n",
    "    transcript_path = row[\"potentialOutPath\"]\n",
    "    if os.path.isfile(transcript_path):\n",
    "        transcript = pd.read_csv(transcript_path, usecols=['start', 'end', 'content'])\n",
    "        chunks = split_transcript(transcript)\n",
    "        if chunks:\n",
    "            x_before_pol.extend(chunks)\n",
    "            y_before_pol.extend([row[\"is_politics\"]] * len(chunks))\n",
    "            row['transcribed'] = 1\n",
    "\n",
    "# is_news:\n",
    "for index, row in df_before_news.iterrows():\n",
    "    transcript_path = row[\"potentialOutPath\"]\n",
    "    if os.path.isfile(transcript_path):\n",
    "        transcript = pd.read_csv(transcript_path, usecols=['start', 'end', 'content'])\n",
    "        chunks = split_transcript(transcript)\n",
    "        if chunks:\n",
    "            x_before_news.extend(chunks)\n",
    "            y_before_news.extend([row[\"is_news\"]] * len(chunks))\n",
    "            row['transcribed'] = 1\n",
    "\n",
    "# Flatten the lists:\n",
    "x_before_pol = [item for chunk in x_before_pol for item in chunk]\n",
    "x_before_news = [item for chunk in x_before_news for item in chunk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a76b9735-092c-4591-8c5b-a98d023ebf5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUnnecessary when using Keras\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Unnecessary when using Keras\n",
    "'''\n",
    "# x_train_pol, x_other_pol, y_train_pol, y_other_pol = train_test_split(\n",
    "#     x_before_pol, y_before_pol, test_size=0.3, random_state=387)\n",
    "\n",
    "# x_dev_pol, x_test_pol, y_dev_pol, y_test_pol = train_test_split(\n",
    "#     x_other_pol, y_other_pol, test_size=(2/3), random_state=387)\n",
    "\n",
    "# x_train_news, x_other_news, y_train_news, y_other_news = train_test_split(\n",
    "#     x_before_news, y_before_news, test_size=0.3, random_state=387)\n",
    "\n",
    "# x_dev_news, x_test_news, y_dev_news, y_test_news = train_test_split(\n",
    "#     x_other_news, y_other_news, test_size=(2/3), random_state=387)\n",
    "\n",
    "# x_train_pol = [item for sublist in x_train_pol for item in sublist]\n",
    "# x_dev_pol = [item for sublist in x_dev_pol for item in sublist]\n",
    "# x_test_pol = [item for sublist in x_test_pol for item in sublist]\n",
    "\n",
    "# x_train_news = [item for sublist in x_train_news for item in sublist]\n",
    "# x_dev_news = [item for sublist in x_dev_news for item in sublist]\n",
    "# x_test_news = [item for sublist in x_test_news for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc4ffa90-dffd-44d1-9a04-db4925f47170",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = 1000 or so (1 or 5% of documents should have that document) \n",
    "We can make it more imbalanced: 4 : 1. 4 is non-news, \n",
    "\n",
    "count_vect_12 = CountVectorizer(ngram_range=(1, 2))\n",
    "count_vect_23 = CountVectorizer(ngram_range=(2, 3))\n",
    "\n",
    "x_12_pol = count_vect_12.fit_transform(x_before_pol)\n",
    "x_23_pol = count_vect_23.fit_transform(x_before_pol)\n",
    "x_12_news = count_vect_12.fit_transform(x_before_news)\n",
    "x_23_news = count_vect_23.fit_transform(x_before_news)\n",
    "\n",
    "# is_pol:\n",
    "# ngrams=(1,2):\n",
    "\n",
    "# x_dev_12_pol = count_vect_12.transform(x_dev_pol)\n",
    "# x_test_12_pol = count_vect_12.transform(x_test_pol)\n",
    "\n",
    "# ngrams=(2,3):\n",
    "# x_dev_23_pol = count_vect_23.transform(x_dev_pol)\n",
    "# x_test_23_pol = count_vect_23.transform(x_test_pol)\n",
    "\n",
    "# is_news:\n",
    "# ngrams=(1,2):\n",
    "# x_dev_12_news = count_vect_12.transform(x_dev_news)\n",
    "# x_test_12_news = count_vect_12.transform(x_test_news)\n",
    "\n",
    "# ngrams=(2,3):\n",
    "# x_dev_23_news = count_vect_23.transform(x_dev_news)\n",
    "# x_test_23_news = count_vect_23.transform(x_test_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deec27e8-c47b-4487-a390-ceaffba5fcf2",
   "metadata": {},
   "source": [
    "#### *1.6.2 Bert models: Dataframes and datasets/dataDicts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5d0b4574-cacc-4360-b677-701234683e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split function for Bert-alike models\n",
    "def split_transcript_bert(text):\n",
    "    transcript['content'] = transcript['content'].fillna('').astype(str)\n",
    "    text = ''\n",
    "    for ind, row in transcript.iterrows():\n",
    "        text += str(row['content'])\n",
    "    return text\n",
    "\n",
    "\n",
    "df_before_pol_bert = pd.DataFrame(columns=['label', 'text'])\n",
    "df_before_news_bert = pd.DataFrame(columns=['label', 'text'])\n",
    "\n",
    "for index, row in df_before_pol.iterrows():\n",
    "    transcript_path = row[\"potentialOutPath\"]\n",
    "    if os.path.isfile(transcript_path):\n",
    "        transcript = pd.read_csv(transcript_path, usecols=['start', 'end', 'content'])\n",
    "        label = row['is_politics']\n",
    "        text = split_transcript_bert(transcript)\n",
    "        if text:\n",
    "            df_before_pol_bert.loc[index] = [label, text]\n",
    "            row['transcribed'] = 1\n",
    "\n",
    "for index, row in df_before_news.iterrows():\n",
    "    transcript_path = row[\"potentialOutPath\"]\n",
    "    if os.path.isfile(transcript_path):\n",
    "        transcript = pd.read_csv(transcript_path, usecols=['start', 'end', 'content'])\n",
    "        label = row['is_news']\n",
    "        text = split_transcript_bert(transcript)\n",
    "        if text:\n",
    "            df_before_news_bert.loc[index] = [label, text]\n",
    "            row['transcribed'] = 1\n",
    "\n",
    "ds_before_pol_bert = Dataset.from_pandas(df_before_pol_bert)\n",
    "ds_before_news_bert = Dataset.from_pandas(df_before_news_bert)\n",
    "\n",
    "ds_before_pol_bert = ds_before_pol_bert.train_test_split(test_size=0.3, seed=387)\n",
    "ds_before_news_bert = ds_before_news_bert.train_test_split(test_size=0.3, seed=387)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505cb12c-92dc-4399-9ada-92fcddd1352d",
   "metadata": {},
   "source": [
    "### *1.7 Obtain inFMonth data (Not now b/c takes too long and not sure of its purpose)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "880bbd86-b6da-440c-8283-bcad6372efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_in_pol = []\n",
    "# y_in_pol = []\n",
    "# x_in_news = []\n",
    "# y_in_news = []\n",
    "\n",
    "# for index, row in df_in_pol.iterrows():\n",
    "#     transcript_path = row[\"potentialOutPath\"]\n",
    "#     if os.path.isfile(transcript_path):\n",
    "#         transcript = pd.read_csv(transcript_path, usecols=['start', 'end', 'content'])\n",
    "#         chunks = split_transcript(transcript)\n",
    "#         x_in_pol.extend(chunks)\n",
    "#         y_in_pol.extend([row[\"is_politics\"]] * len(chunks))\n",
    "#         row['transcribed'] = 1\n",
    "\n",
    "# for index, row in df_in_news.iterrows():\n",
    "#     transcript_path = row[\"potentialOutPath\"]\n",
    "#     if os.path.isfile(transcript_path):\n",
    "#         transcript = pd.read_csv(transcript_path, usecols=['start', 'end', 'content'])\n",
    "#         chunks = split_transcript(transcript)\n",
    "#         x_in_news.extend(chunks)\n",
    "#         y_in_news.extend([row[\"is_news\"]] * len(chunks))\n",
    "#         row['transcribed'] = 1\n",
    "\n",
    "# x_in_pol = [item for sublist in x_in_pol for item in sublist]\n",
    "# x_in_news = [item for sublist in x_in_news for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c466399e-c140-49a0-83d5-86d2cb12925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_in_pol_bert = pd.DataFrame(columns=['label', 'text'])\n",
    "# df_in_news_bert = pd.DataFrame(columns=['label', 'text'])\n",
    "\n",
    "# for index, row in df_in_pol.iterrows():\n",
    "#     transcript_path = row[\"potentialOutPath\"]\n",
    "#     if os.path.isfile(transcript_path):\n",
    "#         transcript = pd.read_csv(transcript_path, usecols=['start', 'end', 'content'])\n",
    "#         label = row['is_politics']\n",
    "#         text = split_transcript_bert(transcript)\n",
    "#         df_in_pol_bert.loc[index] = [label, text]\n",
    "#         row['transcribed'] = 1\n",
    "\n",
    "# for index, row in df_in_news.iterrows():\n",
    "#     transcript_path = row[\"potentialOutPath\"]\n",
    "#     if os.path.isfile(transcript_path):\n",
    "#         transcript = pd.read_csv(transcript_path, usecols=['start', 'end', 'content'])\n",
    "#         label = row['is_news']\n",
    "#         text = split_transcript_bert(transcript)\n",
    "#         df_in_news_bert.loc[index] = [label, text]\n",
    "#         row['transcribed'] = 1\n",
    "\n",
    "# ds_in_pol_bert = Dataset.from_pandas(df_in_pol_bert)\n",
    "# ds_in_news_bert = Dataset.from_pandas(df_in_news_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a794f96e-47cc-4313-acdd-843a1b0c1763",
   "metadata": {},
   "source": [
    "### 2. Model Training! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115711f4-1e56-41eb-84f6-1f6bd5f30e22",
   "metadata": {},
   "source": [
    "#### *2.1 Logistic regression*\n",
    "- To optimize memory usage, I will use Keras instead of scikit-learn to build my models\n",
    "- At this time, my metric of focus is accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "38650e42-1d2a-4a32-ba56-d595311283d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if x_train_12_pol.shape[1] = x_train_23_pol.shape[1] = x_train_pol.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "93848d5e-82fb-4097-bc81-dfcb86e70af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "log_12_pol = Sequential()\n",
    "log_12_pol.add(InputLayer(input_shape=(x_12_pol.shape[1], )))\n",
    "log_12_pol.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "log_12_pol.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = log_12_pol.fit(x_12_pol.toarray(), np.array(y_before_pol), epochs=50, batch_size=8, validation_split=0.3, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c396dac-116c-4f2e-83f0-5ce9f7c63be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed445227-ef83-4dba-bca9-7afe31c97845",
   "metadata": {},
   "source": [
    "#### *2.2 Bert-base*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b64af0-6e9a-443f-8e78-4155474834cf",
   "metadata": {},
   "source": [
    "#### *2.2.1 Tokenize*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f59b8ec9-f5f6-4e38-ac2c-64c6dfadf70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': [1, 0],\n",
       " 'text': [[['yo ancient texan sit sunday morning think first sunday may maryland rain last night yard green tree start butt']],\n",
       "  [['right welcome welcome welcome talk lately may join panel panel panel']]],\n",
       " '__index_level_0__': [697, 4919]}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_before_pol_bert['train'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d88f599d-8303-4208-bb8b-33252ae04150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586a7b9dcf004a36b3c65791c4e229bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function\u001b[39m(examples):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m], padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m tokenized_before_pol \u001b[38;5;241m=\u001b[39m ds_before_pol_bert\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m tokenized_before_news \u001b[38;5;241m=\u001b[39m ds_before_news_bert\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datasets/dataset_dict.py:868\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 868\u001b[0m     {\n\u001b[1;32m    869\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    870\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[1;32m    871\u001b[0m             with_indices\u001b[38;5;241m=\u001b[39mwith_indices,\n\u001b[1;32m    872\u001b[0m             with_rank\u001b[38;5;241m=\u001b[39mwith_rank,\n\u001b[1;32m    873\u001b[0m             input_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[1;32m    874\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[1;32m    875\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    876\u001b[0m             drop_last_batch\u001b[38;5;241m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    877\u001b[0m             remove_columns\u001b[38;5;241m=\u001b[39mremove_columns,\n\u001b[1;32m    878\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    879\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    880\u001b[0m             cache_file_name\u001b[38;5;241m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    881\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    882\u001b[0m             features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    883\u001b[0m             disable_nullable\u001b[38;5;241m=\u001b[39mdisable_nullable,\n\u001b[1;32m    884\u001b[0m             fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs,\n\u001b[1;32m    885\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m    886\u001b[0m             desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[1;32m    887\u001b[0m         )\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    889\u001b[0m     }\n\u001b[1;32m    890\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datasets/dataset_dict.py:869\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    868\u001b[0m     {\n\u001b[0;32m--> 869\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    870\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[1;32m    871\u001b[0m             with_indices\u001b[38;5;241m=\u001b[39mwith_indices,\n\u001b[1;32m    872\u001b[0m             with_rank\u001b[38;5;241m=\u001b[39mwith_rank,\n\u001b[1;32m    873\u001b[0m             input_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[1;32m    874\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[1;32m    875\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    876\u001b[0m             drop_last_batch\u001b[38;5;241m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    877\u001b[0m             remove_columns\u001b[38;5;241m=\u001b[39mremove_columns,\n\u001b[1;32m    878\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    879\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    880\u001b[0m             cache_file_name\u001b[38;5;241m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    881\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    882\u001b[0m             features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    883\u001b[0m             disable_nullable\u001b[38;5;241m=\u001b[39mdisable_nullable,\n\u001b[1;32m    884\u001b[0m             fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs,\n\u001b[1;32m    885\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m    886\u001b[0m             desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[1;32m    887\u001b[0m         )\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    889\u001b[0m     }\n\u001b[1;32m    890\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datasets/arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    593\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    558\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datasets/arrow_dataset.py:3093\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3088\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3089\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3090\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3091\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3092\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3093\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3094\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3095\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datasets/arrow_dataset.py:3470\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3466\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3467\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3468\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3469\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3470\u001b[0m     batch \u001b[38;5;241m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[1;32m   3471\u001b[0m         batch,\n\u001b[1;32m   3472\u001b[0m         indices,\n\u001b[1;32m   3473\u001b[0m         check_same_num_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(shard\u001b[38;5;241m.\u001b[39mlist_indexes()) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   3474\u001b[0m         offset\u001b[38;5;241m=\u001b[39moffset,\n\u001b[1;32m   3475\u001b[0m     )\n\u001b[1;32m   3476\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3477\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3478\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3479\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datasets/arrow_dataset.py:3349\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3348\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3349\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39madditional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs)\n\u001b[1;32m   3350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3351\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3352\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3353\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[83], line 6\u001b[0m, in \u001b[0;36mtokenize_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function\u001b[39m(examples):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m], padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2798\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2797\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2798\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[1;32m   2799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2800\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2856\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2857\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2858\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2859\u001b[0m     )\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2862\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2863\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2865\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_before_pol = ds_before_pol_bert.map(tokenize_function, batched=True)\n",
    "tokenized_before_news = ds_before_news_bert.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f55a254-4228-4f5c-bfdb-afa5e1a32511",
   "metadata": {},
   "source": [
    "#### *2.2.2 Train*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276c1023-2c76-4efd-915d-a945d96e3c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
