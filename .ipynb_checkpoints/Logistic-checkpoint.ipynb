{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b96739e5-a33e-4768-a618-12d9aa14f634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-21 03:41:21.325585: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/anaconda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/anaconda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: sentence-transformers in /opt/anaconda/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/anaconda/lib/python3.10/site-packages (from sentence-transformers) (4.36.2)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda/lib/python3.10/site-packages (from sentence-transformers) (4.64.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/anaconda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda/lib/python3.10/site-packages (from sentence-transformers) (0.16.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda/lib/python3.10/site-packages (from sentence-transformers) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda/lib/python3.10/site-packages (from sentence-transformers) (1.2.1)\n",
      "Requirement already satisfied: scipy in /opt/anaconda/lib/python3.10/site-packages (from sentence-transformers) (1.10.0)\n",
      "Requirement already satisfied: nltk in /opt/anaconda/lib/python3.10/site-packages (from sentence-transformers) (3.7)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda/lib/python3.10/site-packages (from sentence-transformers) (0.1.96)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/anaconda/lib/python3.10/site-packages (from sentence-transformers) (0.20.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (22.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/anaconda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/anaconda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/anaconda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/anaconda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/anaconda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/anaconda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/anaconda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/anaconda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/anaconda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/anaconda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: sympy in /opt/anaconda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/anaconda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/anaconda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/anaconda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence-transformers) (12.3.52)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/anaconda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/anaconda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.7.9)\n",
      "Requirement already satisfied: click in /opt/anaconda/lib/python3.10/site-packages (from nltk->sentence-transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in /opt/anaconda/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda/lib/python3.10/site-packages (from torchvision->sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.2.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/anaconda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/anaconda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/anaconda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords.add('u')\n",
    "stopwords.add('th')\n",
    "sys.path.append(\"/home/bowenyi/.local/lib/python3.11/site-packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2156140-9d81-4823-80de-3d9ef3b5ddf2",
   "metadata": {},
   "source": [
    "### **1. Preprocess dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bd94693-bad7-49d7-91b4-9dc1aef9abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before = pd.read_csv(\"/shared/3/projects/benlitterer/podcastData/processed/beforeFloydMonth/beforeFMonth.tsv\", lineterminator = '\\n', low_memory=False)\n",
    "df_in = pd.read_csv(\"/shared/3/projects/benlitterer/podcastData/processed/floydMonth/floydMonthEnSHORT.csv\", lineterminator = '\\n', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f37e155-b62b-453b-91d3-1a79c99634db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before = df_before.dropna(subset=['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10'], how='all')\n",
    "df_before = df_before.drop_duplicates()\n",
    "df_before = df_before.drop_duplicates(subset=['potentialOutPath'])\n",
    "\n",
    "df_in = df_in.dropna(subset=['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10'], how='all')\n",
    "df_in = df_in.drop_duplicates()\n",
    "df_in = df_in.drop_duplicates(subset=['potentialOutPath'])\n",
    "\n",
    "df_before['potentialOutPath'] = df_before['potentialOutPath'].apply(lambda x: \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/beforeFMonth\" + x)\n",
    "df_in['potentialOutPath'] = df_in['potentialOutPath'].apply(lambda x: \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/floydMonth\" + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b1dda6-060e-41d3-bf4a-420ace9d0c8c",
   "metadata": {},
   "source": [
    "#### **1.1 Introduce an is_news column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2000e87b-b775-4c03-906a-9fb96c7f08bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "df_before['is_news'] = df_before[['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10']].apply(lambda x: 1 if 'news' in x.values else 0, axis=1)\n",
    "df_in['is_news'] = df_in[['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10']].apply(lambda x: 1 if 'news' in x.values else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142d1c6d-39a5-40ff-8882-6d5058e92c2d",
   "metadata": {},
   "source": [
    "#### **1.2 Downsample the training data to 4:1 distribution**\n",
    "- Training data are df_before_news and df_after_news (unavailable yet)\n",
    "- non-news : news = 4 : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24e8115d-83ac-46c7-bf0d-acef93fcd932",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before_news = df_before[df_before['is_news'] == 1]\n",
    "n_before_news = df_before_news.shape[0]\n",
    "df_before_no_news = df_before[df_before['is_news'] == 0].sample(n=n_before_news*4, replace=False, random_state=387)\n",
    "df_before_news = pd.concat([df_before_news, df_before_no_news], ignore_index=True)\n",
    "df_before_news = df_before_news.sample(frac=1, random_state=387).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec8f5e-44b2-446e-8ddd-7f4ce1be50d6",
   "metadata": {},
   "source": [
    "### **2. Obtain the training and non-training data**\n",
    "- df_train: dataframe for training data\n",
    "- df_non_train: dataframe for non-training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "152bc70c-ee7e-4163-a3a1-a807522f1722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"\\n\", \"\", text)  # remove line breaks\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # remove [Music], (Audio), etc.\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    text = text.lower()    # convert to lowercase\n",
    "    text = re.sub(r'\\b\\w+\\.com\\b', '', text)  # remove something.com\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)   # remove hyperlinks\n",
    "    text = re.sub(r\"\\d+\", \"\", text)   # remove digits and currencies \n",
    "    text = re.sub(r'[\\$\\d+\\d+\\$]', \"\", text)\n",
    "    text = re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', text)   # remove dates\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r' ', text)  # remove non-ascii\n",
    "    text = contractions.fix(text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)   # remove punctuation\n",
    "\n",
    "    filtered_tokens = [word for word in word_tokenize(text) if not word in stopwords]\n",
    "    pos_tags = pos_tag(filtered_tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags if lemmatizer.lemmatize(word, get_wordnet_pos(tag)) not in stopwords]\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "\n",
    "    return lemmatized_text\n",
    "\n",
    "\n",
    "def split_transcript(transcript):\n",
    "    transcript['content'] = transcript['content'].fillna('').astype(str)\n",
    "    start_time = 0\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    end_of_sentence = ['.', '!', '?', ']', ')']\n",
    "\n",
    "    for index, row in transcript.iterrows():\n",
    "        content = str(row['content'])\n",
    "        if content.strip() != '':\n",
    "            if row['end'] - start_time < 60:\n",
    "                chunk += content\n",
    "            else:\n",
    "                chunk += content\n",
    "                if any(ele in content for ele in end_of_sentence):\n",
    "                    chunk = preprocess_text(chunk)\n",
    "                    if chunk.strip() != '':\n",
    "                        chunks.append(chunk)\n",
    "                        start_time = row['end']\n",
    "                    chunk = \"\"\n",
    "\n",
    "    if len(chunk) != 0:\n",
    "        chunk = preprocess_text(chunk)\n",
    "        if chunk.strip() != '':\n",
    "            chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245c9d03-9539-4275-ab96-702e45722929",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.DataFrame(columns=['X', 'y', 'real_y', 'prob', 'path'])\n",
    "for index, row in df_before_news.iterrows():\n",
    "    path = row[\"potentialOutPath\"]\n",
    "    if os.path.isfile(path):\n",
    "        transcript = pd.read_csv(path, usecols=['start', 'end', 'content'])\n",
    "        X = split_transcript(transcript)\n",
    "        y = row['is_news']\n",
    "        real_y = -1\n",
    "        prob = -1\n",
    "        df_news.loc[index] = [X, y, real_y, prob, path]\n",
    "\n",
    "df_news = df_news.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd1659e-5156-4965-b6ea-e555321c0e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = df_news[df_news['X'].apply(lambda x: len(x) > 0)]\n",
    "df_train, df_non_train = train_test_split(df_news, test_size=0.5, random_state=1)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_non_train = df_non_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd6d4c-05ba-456a-bf48-e353337dfc43",
   "metadata": {},
   "source": [
    "### **3. Model training**\n",
    "- Logistic regression (l2-loss, min_df = 0.025)\n",
    "- Unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a905e4b-ad43-4ebe-9433-536fb48490ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = [], []\n",
    "\n",
    "for index, row in df_train.iterrows():\n",
    "    for chunk in row['X']:\n",
    "        X_train.append(chunk)\n",
    "    y_train.extend([row['y']] * len(row['X']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c893c864-070d-4669-a22b-33ddcb2b6378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), min_df=0.025)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "log_reg = LogisticRegression(random_state=1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9af179-aa07-4b48-9d75-a3acde3084f9",
   "metadata": {},
   "source": [
    "### **4. Model calibration on non-train data**\n",
    "- Predict probabilities on each row, store output probabilities into \"prob\"\n",
    "- Build strata profile\n",
    "- Annotate 10 random samples from each stratum\n",
    "- Calculate F-1 for each cutoff (9 in total)\n",
    "- Find the best cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc426ad-955a-47b2-900e-4fb4d552f55a",
   "metadata": {},
   "source": [
    "#### **4.1 Predict probabilities & fill up the \"prob\" column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b1eba4-4391-4579-86db-5309e79d99d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.transform(df_non_train['X'])\n",
    "y = log_reg.predict_proba(X)\n",
    "df_non_train.loc[:, 'prob'] = y[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c475b4-6b8c-4abe-aa16-e57e43988079",
   "metadata": {},
   "source": [
    "#### **4.2 Build strata profile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5d67d5-5ab6-4faf-9ba3-c002a43dbedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_train['stratum'] = pd.cut(df_non_train['prob'], bins=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], \n",
    "                       labels=['0-10%', '10-20%', '20-30%', '30-40%', '40-50%', '50-60%', '60-70%', '70-80%', '80-90%', '90-100%'], \n",
    "                       include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa98115a-06ce-4b92-b59c-2f7e9a9d8dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution across different strata\n",
    "df_non_train['stratum'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31188ec6-039e-4cab-ada1-2c144e4b4c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame according to the order of strata\n",
    "df_non_train = df_non_train.sort_values(by='stratum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7245e3a-bdb9-4461-9801-c5226069a881",
   "metadata": {},
   "source": [
    "#### **4.3 Annotate 10 random samples from each stratum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f85440-fc23-498b-8ced-facf6248bf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "annote_df = df_non_train.groupby('stratum').apply(lambda x: x.sample(n=10, random_state=1)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db9fac4-5264-4900-a9d5-abed8000256c",
   "metadata": {},
   "source": [
    "#### **4.4 Calculate F-1 for each cutoff (9 in total)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca3b51a-c5c9-4be0-ae14-6cf6eb469322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d250d67f-6718-4586-8034-002455b29f07",
   "metadata": {},
   "source": [
    "#### **4.5 Find the best cutoff and compare the inter-model performance**\n",
    "- logistic regression (this file)\n",
    "- miniLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f44eaa-800c-4170-8a47-cd927a166ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
