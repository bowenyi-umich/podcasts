{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118572f8-4912-4eac-b812-4f6762dff0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in /home/bowenyi/.local/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/anaconda/lib/python3.11/site-packages (from sentence-transformers) (4.35.2)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda/lib/python3.11/site-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/anaconda/lib/python3.11/site-packages (from sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: torchvision in /home/bowenyi/.local/lib/python3.11/site-packages (from sentence-transformers) (0.16.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda/lib/python3.11/site-packages (from sentence-transformers) (1.26.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda/lib/python3.11/site-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda/lib/python3.11/site-packages (from sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: nltk in /opt/anaconda/lib/python3.11/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /home/bowenyi/.local/lib/python3.11/site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/anaconda/lib/python3.11/site-packages (from sentence-transformers) (0.19.4)\n",
      "Requirement already satisfied: filelock in /opt/anaconda/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/bowenyi/.local/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: sympy in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/anaconda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/anaconda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence-transformers) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/anaconda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/anaconda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.2)\n",
      "Requirement already satisfied: click in /opt/anaconda/lib/python3.11/site-packages (from nltk->sentence-transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda/lib/python3.11/site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda/lib/python3.11/site-packages (from torchvision->sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda/lib/python3.11/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-21 00:27:16.704285: I tensorflow/core/platform/cpu_feature_guard.cc:181] Beginning TensorFlow 2.15, this package will be updated to install stock TensorFlow 2.15 alongside Intel's TensorFlow CPU extension plugin, which provides all the optimizations available in the package and more. If a compatible version of stock TensorFlow is present, only the extension will get installed. No changes to code or installation setup is needed as a result of this change.\n",
      "More information on Intel's optimizations for TensorFlow, delivered as TensorFlow extension plugin can be viewed at https://github.com/intel/intel-extension-for-tensorflow.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f4bd23-619d-4cae-8e89-360c2768a37b",
   "metadata": {},
   "source": [
    "### **1. Preprocess dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "294c419b-3bbc-4a76-9065-347c91c49f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before = pd.read_csv(\"/shared/3/projects/benlitterer/podcastData/processed/beforeFloydMonth/beforeFMonth.tsv\", lineterminator = '\\n', low_memory=False)\n",
    "df_in = pd.read_csv(\"/shared/3/projects/benlitterer/podcastData/processed/floydMonth/floydMonthEnSHORT.csv\", lineterminator = '\\n', low_memory=False)\n",
    "\n",
    "df_before = df_before.dropna(subset=['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10'], how='all')\n",
    "df_before = df_before.drop_duplicates()\n",
    "df_before = df_before.drop_duplicates(subset=['potentialOutPath'])\n",
    "\n",
    "df_in = df_in.dropna(subset=['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10'], how='all')\n",
    "df_in = df_in.drop_duplicates()\n",
    "df_in = df_in.drop_duplicates(subset=['potentialOutPath'])\n",
    "\n",
    "df_before['potentialOutPath'] = df_before['potentialOutPath'].apply(lambda x: \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/beforeFMonth\" + x)\n",
    "df_in['potentialOutPath'] = df_in['potentialOutPath'].apply(lambda x: \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/floydMonth\" + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fba2d5-3344-49d1-8b1a-e724fee35a46",
   "metadata": {},
   "source": [
    "#### **1.1 Introduce an is_news column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a8a07ed-be4c-4437-b7da-9ef21e6c17ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "df_before['is_news'] = df_before[['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10']].apply(lambda x: 1 if 'news' in x.values else 0, axis=1)\n",
    "df_in['is_news'] = df_in[['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10']].apply(lambda x: 1 if 'news' in x.values else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200160c2-27cf-4777-ba6f-9ae4b6c1b50c",
   "metadata": {},
   "source": [
    "#### **1.2 Downsample the training data to 4:1 distribution**\n",
    "- Training data are df_before_news and df_after_news (unavailable yet)\n",
    "- non-news : news = 4 : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d06ca0a6-553a-472f-812c-be76d6a2671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before_news = df_before[df_before['is_news'] == 1]\n",
    "n_before_news = df_before_news.shape[0]\n",
    "df_before_no_news = df_before[df_before['is_news'] == 0].sample(n=n_before_news*4, replace=False, random_state=387)\n",
    "df_before_news = pd.concat([df_before_news, df_before_no_news], ignore_index=True)\n",
    "df_before_news = df_before_news.sample(frac=1, random_state=387).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6ed85a-7450-4c39-9592-13f751b09d9a",
   "metadata": {},
   "source": [
    "### **2. Obtain the training and non-training data**\n",
    "- df_train: dataframe for training data\n",
    "- df_non_train: dataframe for non-training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6185aff4-d56f-4b2e-98c0-9b7b2948deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_transcript(transcript):\n",
    "    transcript['content'] = transcript['content'].fillna('').astype(str)\n",
    "    text = ''\n",
    "    for ind, row in transcript.iterrows():\n",
    "        text += str(row['content'])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22a890b-629f-481a-a69b-3da8b5932336",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.DataFrame(columns=['X', 'y', 'real_y', 'prob', 'path'])\n",
    "for index, row in df_before_news.iterrows():\n",
    "    path = row[\"potentialOutPath\"]\n",
    "    if os.path.isfile(path):\n",
    "        transcript = pd.read_csv(path, usecols=['start', 'end', 'content'])\n",
    "        X = split_transcript(transcript)\n",
    "        y = row['is_news']\n",
    "        real_y = -1\n",
    "        prob = -1\n",
    "        df_news.loc[index] = [X, y, real_y, prob, path]\n",
    "\n",
    "df_news = df_news.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53f807d-08c4-488c-93e5-67291634db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = df_news[df_news['X'].apply(lambda x: len(x) > 0)]\n",
    "df_train, df_non_train = train_test_split(df_news, test_size=0.5, random_state=1)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_non_train = df_non_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145144a4-0418-4684-8288-a801bf62cfbd",
   "metadata": {},
   "source": [
    "### **3. Model training**\n",
    "- MiniLM (microsoft/MiniLM-L12-H384-uncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa5c76e-6a8a-405d-a46f-20acceb1a703",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_miniLM = pd.DataFrame(columns=['label', 'text'])\n",
    "for index, row in df_train.iterrows():\n",
    "    df_train_miniLM.loc[index] = [row['y'], row['X']]\n",
    "\n",
    "ds_train_miniLM = Dataset.from_pandas(df_train_miniLM)\n",
    "ds_train_miniLM = ds_train_miniLM.remove_columns([\"__index_level_0__\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2941d9e-011a-4937-8349-1350418eb3d4",
   "metadata": {},
   "source": [
    "### **Note: May change the following parameters as per David's advice:**\n",
    "- max_length=512 in tokenizer()\n",
    "- per_device_train_batch_size=64 in TrainingArguments()\n",
    "- num_train_epochs=3 in TrainingArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5324bf8b-14c8-4936-a74d-a5d6abffa689",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/MiniLM-L12-H384-uncased\")\n",
    "miniLM = AutoModelForSequenceClassification.from_pretrained(\"microsoft/MiniLM-L12-H384-uncased\", num_labels=2)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "tokenized_datasets = ds_train_miniLM.map(tokenize_function, batched=True)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", per_device_train_batch_size=64, num_train_epochs=3)\n",
    "trainer = Trainer(\n",
    "    model=miniLM,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89806ea8-c8f6-4312-80ba-2ad316672b7b",
   "metadata": {},
   "source": [
    "### **4. Model calibration on non-train data**\n",
    "- Predict probabilities on each row & fill up \"prob\" column\n",
    "- Build strata profile\n",
    "- Annotate 10 random samples from each stratum\n",
    "- Calculate F-1 for each cutoff (9 in total)\n",
    "- Find the best cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1460b294-8e2f-459c-8ac9-8a4b1bf67bcf",
   "metadata": {},
   "source": [
    "#### **4.1 Predict probabilities & fill up the \"prob\" column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a18654-58e9-428d-b36f-54b74fd20b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "prediction_inputs = df_non_train['X'].tolist()\n",
    "predict_dset = Dataset.from_pandas(pd.DataFrame({\"text\": prediction_inputs}))\n",
    "predict_dset = predict_dset.remove_columns([\"__index_level_0__\"])\n",
    "\n",
    "tokenized_predict_dset = predict_dset.map(tokenize_function, batched=True)\n",
    "predictions = trainer.predict(tokenized_predict_dset)\n",
    "\n",
    "# Apply softmax to the model's raw predictions to get probabilities\n",
    "probs = softmax(predictions.predictions, axis=1)[:, 1]\n",
    "\n",
    "df_non_train.loc[:, 'prob'] = probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1268dd-ce6c-4496-9bb8-ff7889e7c3b2",
   "metadata": {},
   "source": [
    "#### **4.2 Build strata profile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4f54ad-1955-44b2-a07a-2b50f531a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_train['stratum'] = pd.cut(df_non_train['prob'], bins=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], \n",
    "                       labels=['0-10%', '10-20%', '20-30%', '30-40%', '40-50%', '50-60%', '60-70%', '70-80%', '80-90%', '90-100%'], \n",
    "                       include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e350421b-98a4-400b-9f8e-7468255d588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution across different strata\n",
    "df_non_train['stratum'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0d1f5a-df72-4ec7-9f12-343f8a639ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame according to the order of strata\n",
    "df_non_train = df_non_train.sort_values(by='stratum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c843565c-6a2c-4072-b0af-146fe6f28311",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **4.3 Annotate 10 random samples from each stratum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc95f748-fa5b-4409-bc34-f6db60fcc3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "annote_df = df_non_train.groupby('stratum').apply(lambda x: x.sample(n=10, random_state=1)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d5d07-cdcb-4330-a99d-af29c27acbed",
   "metadata": {},
   "source": [
    "#### **4.4 Calculate F-1 for each cutoff (9 in total)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad29ea6-96f7-4542-9fb2-45f890879fab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03bae5f2-1b2c-47a7-a9e1-0727fde7644b",
   "metadata": {},
   "source": [
    "#### **4.5 Find the best cutoff and compare inter-model performance**\n",
    "- miniLM (this file)\n",
    "- logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1000b6f2-f6af-4fbd-8a39-02153fc15e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
