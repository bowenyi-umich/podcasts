{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "421386d5-d76c-4b6b-83a6-714c278eebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118572f8-4912-4eac-b812-4f6762dff0f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-07 01:01:23.754743: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import re\n",
    "import pycld2 as cld2  # language recognition\n",
    "import os\n",
    "\n",
    "from scipy.special import softmax\n",
    "from datasets import Dataset , load_metric\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer \n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Let Jupyter Notebook show more content under cell\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "metric = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1a0e0d-9965-4335-9929-f8dd53f0426a",
   "metadata": {},
   "source": [
    "### **1. Obtain the training and non-training data**\n",
    "- df_before: \"path\", \"content\", \"is_news\" as columns\n",
    "- Clean the corpus\n",
    "  - music/audio tags\n",
    "  - transcribing typos: Replace triple-double quotes with a single double quote. Replace single double quotes with commas\n",
    "  - remove \"content\" at the beginning of every transcript\n",
    "  - Some transcripts are non-sense due to an attempt to transcribe foreign languages. Can be excluded/discarded. **(resolved)**\n",
    "  - **Remove transcripts with non-English words**\n",
    "- Export the cleaned dataframe to a .csv file\n",
    "- Downsample the training corpus\n",
    "- Chunkify transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8aa74a8-c6cd-4110-a755-4bea4d7bf847",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before = pd.read_json('/shared/3/projects/benlitterer/podcastData/processed/beforeFloydMonth/5_4_5_10.jsonl', orient=\"records\", lines=True)\n",
    "df_before = df_before.dropna(subset=['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10'], how='all')\n",
    "df_before = df_before.drop_duplicates()\n",
    "df_before = df_before.drop_duplicates(subset=['potentialOutPath'])\n",
    "df_before['potentialOutPath'] = df_before['potentialOutPath'].apply(lambda x: \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/5_4_5_10\" + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4e12c33-13d7-4828-92d4-7b776be371c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_after = pd.read_json('/shared/3/projects/benlitterer/podcastData/processed/afterFloydMonth/6_9_6_15.jsonl', orient=\"records\", lines=True)\n",
    "df_after = df_after.dropna(subset=['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10'], how='all')\n",
    "df_after = df_after.drop_duplicates()\n",
    "df_after = df_after.drop_duplicates(subset=['potentialOutPath'])\n",
    "df_after['potentialOutPath'] = df_after['potentialOutPath'].apply(lambda x: \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/6_9_6_15\" + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb033bcc-dfe3-4c03-b894-c63b8054a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = pd.read_csv(\"/shared/3/projects/benlitterer/podcastData/processed/floydMonth/floydMonthEn.csv\", lineterminator = '\\n', low_memory=False)\n",
    "df_in = df_in.dropna(subset=['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10'], how='all')\n",
    "df_in = df_in.drop_duplicates()\n",
    "df_in = df_in.drop_duplicates(subset=['potentialOutPath'])\n",
    "df_in['potentialOutPath'] = df_in['potentialOutPath'].apply(lambda x: \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/floydMonth\" + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7379bce3-8156-4fd0-a04f-ca0d602377fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "df_before['is_news'] = df_before[['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10']].apply(lambda x: 1 if 'news' in x.values else 0, axis=1)\n",
    "df_after['is_news'] = df_after[['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10']].apply(lambda x: 1 if 'news' in x.values else 0, axis=1)\n",
    "df_in['is_news'] = df_in[['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10']].apply(lambda x: 1 if 'news' in x.values else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1265f07-ceae-4568-8879-0015aa1853f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# included pubDate to help us define news\n",
    "after_df = pd.DataFrame(columns=['text', 'label', 'path', 'pubDate'])\n",
    "in_df = pd.DataFrame(columns=['text', 'label', 'path', 'pubDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3ff9f498-b4be-4534-a7dc-d1d4bd564b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_transcript(transcript):\n",
    "    chunk_size = 100\n",
    "    transcript['content'] = transcript['content'].fillna('').astype(str)\n",
    "    text = ''.join(transcript['content'].astype(str))\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    text = re.sub(r'\\*.*?\\*', '', text)\n",
    "    \n",
    "    \n",
    "    sentences = re.split(r'(?<=[.!?]) ', text)\n",
    "    sentences = [sentence + ' ' for sentence in sentences]\n",
    "    chunks = []\n",
    "    current_chunk = ''\n",
    "\n",
    "    for ind, sentence in enumerate(sentences):\n",
    "        if len(current_chunk.split(' ')) + len(sentence.split(' ')) > chunk_size:\n",
    "            if ind != len(sentences) - 2:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = sentence\n",
    "            else:\n",
    "                current_chunk += sentences[ind]  \n",
    "                current_chunk += sentences[ind + 1]  \n",
    "                chunks.append(current_chunk)  \n",
    "                break\n",
    "        else:\n",
    "            current_chunk += sentence\n",
    "    else:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a35b077-250d-4980-8cee-657bdf057068",
   "metadata": {},
   "source": [
    "#### Build dataframes of labeled chunks:\n",
    "#### **Before:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "005e1255-ca2a-4824-9642-ef1d4545c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_text = []\n",
    "before_label = []\n",
    "before_path = []\n",
    "before_date = []\n",
    "\n",
    "for index, row in df_before.iterrows():\n",
    "    path = row[\"potentialOutPath\"]\n",
    "    if os.path.isfile(path):\n",
    "        transcript = pd.read_csv(path, usecols=['content'])\n",
    "        chunks = split_transcript(transcript)\n",
    "        if chunks:\n",
    "            before_text.extend(chunks)\n",
    "            before_label.extend([row[\"is_news\"]] * len(chunks))\n",
    "            before_path.extend([path] * len(chunks))\n",
    "            before_date.extend([row[\"pubDate\"]] * len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "daaf543b-e2a1-4f81-9cd0-00bc2f99d2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'text': before_text,\n",
    "    'label': before_label,\n",
    "    'path': before_path,\n",
    "    'pubDate': before_date\n",
    "}\n",
    "\n",
    "before_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "12df8b22-7ab7-4d23-b31a-bcbbc938b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_df = before_df[before_df['text'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dc5985-724f-4235-9be6-11b97ce80c1d",
   "metadata": {},
   "source": [
    "#### **After:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f5280ebb-977e-410c-b66d-a7298db0a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_text = []\n",
    "after_label = []\n",
    "after_path = []\n",
    "after_date = []\n",
    "\n",
    "for index, row in df_after.iterrows():\n",
    "    path = row[\"potentialOutPath\"]\n",
    "    if os.path.isfile(path):\n",
    "        transcript = pd.read_csv(path, usecols=['content'])\n",
    "        chunks = split_transcript(transcript)\n",
    "        if chunks:\n",
    "            after_text.extend(chunks)\n",
    "            after_label.extend([row[\"is_news\"]] * len(chunks))\n",
    "            after_path.extend([path] * len(chunks))\n",
    "            after_date.extend([row[\"pubDate\"]] * len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "68f427f4-5c14-49ce-b0a5-80f2d145ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'text': after_text,\n",
    "    'label': after_label,\n",
    "    'path': after_path,\n",
    "    'pubDate': after_date\n",
    "}\n",
    "\n",
    "after_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c0aece53-adf5-411c-abfc-200840e3a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_df = after_df[after_df['text'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80e9c65-f112-48f7-b2e9-9881fe94d478",
   "metadata": {},
   "source": [
    "#### **In:**\n",
    "### **In the code block below, remove index <= 5000 once data collection is done**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0d178c6e-e972-4474-9dde-d0a30ef9b5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_text = []\n",
    "in_label = []\n",
    "in_path = []\n",
    "in_date = []\n",
    "\n",
    "for index, row in df_in.iterrows():\n",
    "    if index <= 5000:\n",
    "        path = row[\"potentialOutPath\"]\n",
    "        if os.path.isfile(path):\n",
    "            transcript = pd.read_csv(path, usecols=['content'])\n",
    "            chunks = split_transcript(transcript)\n",
    "            if chunks:\n",
    "                in_text.extend(chunks)\n",
    "                in_label.extend([row[\"is_news\"]] * len(chunks))\n",
    "                in_path.extend([path] * len(chunks))\n",
    "                in_date.extend([row[\"pubDate\"]] * len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8ae3cd67-6004-4bba-86f9-9ccd0f5228d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'text': in_text,\n",
    "    'label': in_label,\n",
    "    'path': in_path,\n",
    "    'pubDate': in_date\n",
    "}\n",
    "\n",
    "df_test = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "53659799-4fcd-4a1e-8865-d94f94632c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test[df_test['text'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e568d1d3-2339-4e75-8fa6-dd337b23e5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([before_df, after_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504dda2a-ae04-4408-8bf1-2db0447333d1",
   "metadata": {},
   "source": [
    "### Remove transcripts that contain non-English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f84636e0-0fed-4926-a9b8-49a30558105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_eng(text):\n",
    "    isReliable, textBytesFound, details, vectors = cld2.detect(text, returnVectors=True)\n",
    "    if details[0][0] != \"ENGLISH\":\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "df_train['is_English'] = df_train['text'].apply(is_eng)\n",
    "df_test['is_English'] = df_test['text'].apply(is_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "57ab37e9-9337-4b68-accb-c9b0d1c297a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train['is_English'] == True]\n",
    "df_test = df_test[df_test['is_English'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "370a93bc-be2d-4a7d-8d94-ff46c6ca109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_before_labeled = df_before_labeled.rename(columns={\"potentialOutPath\": \"path\"})\n",
    "# df_path_and_label = df_before_labeled[[\"path\", \"is_news\"]]\n",
    "# df_path_and_label.drop_duplicates(subset='path', keep='first', inplace=True)\n",
    "# df_before.drop_duplicates(subset='path', keep='first', inplace=True)\n",
    "# df_before = df_before.merge(df_path_and_label, on=\"path\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a707f728-b974-49c1-897b-18042d3c3175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the cleaned corpus to a .csv file:\n",
    "# df_before.to_csv(\"labeled_before_transcripts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b9a3e-6831-4874-92d4-04ec7fd8c522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_rows = df_for_test[df_for_test['is_English'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dfd138-df55-4049-9ee5-ae1943dc0bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = df_before_cp[df_before_cp['is_mistranscribed']].index.tolist()\n",
    "# paths = df_before_cp[df_before_cp['is_mistranscribed']]['path'].tolist()\n",
    "# labels = df_before_cp[df_before_cp['is_mistranscribed']]['label'].tolist()\n",
    "# texts = df_before_cp[df_before_cp['is_mistranscribed']]['content'].tolist()\n",
    "# df_mistranscribed = pd.DataFrame({\"index\":indices, \"path\":paths, \"label\":labels, \"text\":texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0215710a-8e67-452b-8b2c-49defc690b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_before_cp.drop(df_before_cp[~df_before_cp['is_English']].index, inplace=True)\n",
    "# df_before_cp = df_before_cp[df_before_cp['text'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4199d1be-3953-4dd7-8cf1-f8a18bad9932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_length = df_before_cp['text'].apply(len).min()\n",
    "# print(min_length)\n",
    "# index_of_min_length = df_before_cp['text'].apply(len).idxmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b7ee83-f756-4328-9e6d-da2ddfc7f447",
   "metadata": {},
   "source": [
    "## 1.3 Downsampling\n",
    "\n",
    "## **Make sure to do this once have all data collected**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "79e77f84-45b9-4032-8ad5-5c9f9917a083",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_df_news = before_daf[before_dfa['label'] == 1]\n",
    "n_before_news = before_df_news.shape[0]\n",
    "before_df_no_news = before_df[before_df['is_news'] == 0].sample(n=n_before_news*5, replace=False, random_state=1)\n",
    "df_before = pd.concat([df_before_news, df_before_no_news], ignore_index=True)\n",
    "df_before = df_before.sample(frac=1, random_state=387).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15615c6f-79cd-4f9d-b330-681c6a29cc40",
   "metadata": {},
   "source": [
    "### 1.4 Split dataset into training, dev, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7bdaf684-519e-42bd-b409-e38a4af89544",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_non_train = train_test_split(df_train, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5f30a2c7-dc04-4709-931b-237d3b7021e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_train, df_train_test = train_test_split(df_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b3dd6da0-983f-4bab-9502-067285bf2160",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_non_train = Dataset.from_pandas(df_non_train[['text', 'label']])\n",
    "ds_train_test = Dataset.from_pandas(df_train_test[['text', 'label']])\n",
    "ds_train_train = Dataset.from_pandas(df_train_train[['text', 'label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145144a4-0418-4684-8288-a801bf62cfbd",
   "metadata": {},
   "source": [
    "### **2. Model training**\n",
    "- MiniLM (microsoft/MiniLM-L12-H384-uncased)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6454dcb8-2adb-4e28-b010-ec44774138ef",
   "metadata": {},
   "source": [
    "#### **2.1 Prepare and tokenize a dataset**\n",
    "**Note: max_length = 800 because the average length of transcripts is 798 tokens.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9980a021-ecf1-4b01-aec2-fdf71c9ee78b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a52ad821354eacaac9e695fae8673f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53738 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d2ea0841684600b6aff5ca0d2094bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7677 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310ceeb5e2bc4ce988165f97cf809da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15354 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/MiniLM-L12-H384-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], max_length=512, padding=\"max_length\", truncation=True)\n",
    "\n",
    "ds_train_dev = ds_train_dev.map(tokenize_function, batched=True, batch_size=20)\n",
    "ds_test = ds_test.map(tokenize_function, batched=True, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2941d9e-011a-4937-8349-1350418eb3d4",
   "metadata": {},
   "source": [
    "### **Note: May change the following parameters as per David's advice:**\n",
    "- max_length=512 in tokenizer()\n",
    "- per_device_train_batch_size=258 in TrainingArguments()\n",
    "- num_train_epochs=3 in TrainingArguments()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878ac30b-8dae-4cc0-b1c3-5dc14b44bc88",
   "metadata": {},
   "source": [
    "#### **2.2 Model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9a9f0005-c230-4534-9d59-439e78ac3926",
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceNum = 0\n",
    "device = torch.device(\"cuda:\" + str(deviceNum) if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8fc97a81-306d-4ec7-8aca-7088211e408b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"microsoft/MiniLM-L12-H384-uncased\", num_labels=2).to(device)\n",
    "    \n",
    "output_dir = \"/shared/3/projects/bowenyi/Floyd_Month/Content Analysis/podcasts/model_output\"   \n",
    "seed = 1\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=128,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    do_eval=True,\n",
    "    seed=seed,\n",
    "    save_strategy='steps',\n",
    "    save_steps=10_000,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=10_000,\n",
    "    logging_dir=output_dir + 'logs/',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    run_name='podcasts-study' + str(seed)\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,    \n",
    "    args=training_args,\n",
    "    train_dataset=ds_train_dev[\"train\"],\n",
    "    eval_dataset=ds_train_dev[\"test\"],    \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e55cec1-9c24-43f6-925e-18701ab04888",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b9c7c23-a823-4d59-b77f-e230d6963048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 01:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4505256712436676,\n",
       " 'eval_f1': 0.5026443833298075,\n",
       " 'eval_runtime': 92.1173,\n",
       " 'eval_samples_per_second': 166.679,\n",
       " 'eval_steps_per_second': 1.303,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89806ea8-c8f6-4312-80ba-2ad316672b7b",
   "metadata": {},
   "source": [
    "### **3. Model calibration on non-train data**\n",
    "- Predict probabilities on each row & fill up \"prob\" column\n",
    "- Build strata profile\n",
    "- Annotate 10 random samples from each stratum\n",
    "- Calculate F-1 for each cutoff (9 in total)\n",
    "- Find the best cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1460b294-8e2f-459c-8ac9-8a4b1bf67bcf",
   "metadata": {},
   "source": [
    "#### **3.1 Predict probabilities & fill up the \"prob\" column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6b72ce-d40a-430d-ad7e-004c4bdf579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_inputs = df_non_train['raw_X'].tolist()\n",
    "# predict_dset = Dataset.from_pandas(pd.DataFrame({\"text\": prediction_inputs}))\n",
    "\n",
    "# tokenized_predict_dset = predict_dset.map(tokenize_function, batched=True)\n",
    "# predictions = trainer.predict(tokenized_predict_dset)\n",
    "\n",
    "# # Apply softmax to the model's raw predictions to get probabilities\n",
    "# probs = softmax(predictions.predictions, axis=1)[:, 1]\n",
    "\n",
    "# df_non_train.loc[:, 'prob'] = probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1268dd-ce6c-4496-9bb8-ff7889e7c3b2",
   "metadata": {},
   "source": [
    "#### **3.2 Build strata profile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4f54ad-1955-44b2-a07a-2b50f531a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_non_train['stratum'] = pd.cut(df_non_train['prob'], bins=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], \n",
    "#                        labels=['0-10%', '10-20%', '20-30%', '30-40%', '40-50%', '50-60%', '60-70%', '70-80%', '80-90%', '90-100%'], \n",
    "#                        include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e350421b-98a4-400b-9f8e-7468255d588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the distribution across different strata\n",
    "# df_non_train['stratum'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d62af-3366-4cbb-bd1d-fbfddf45d94a",
   "metadata": {},
   "source": [
    "#### Original strata distribution:\n",
    "- 0-10%      20534  \n",
    "- 10-20%      9102\n",
    "- 20-30%      3375\n",
    "- 30-40%      2170\n",
    "- 40-50%      1757\n",
    "- 80-90%      1688\n",
    "- 50-60%      1022\n",
    "- 60-70%       734\n",
    "- 70-80%       699\n",
    "- 90-100%        0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b960d9d3-7830-4015-9c8f-26a2fb731432",
   "metadata": {},
   "source": [
    "#### **Note: Since 90-100% stratum is empty, I will increase all probabilities by 1.15% (Ask David or Ben)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a2510694-6bf1-4e74-b59c-6db8ef7bdd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_non_train['prob'] += 0.0115 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2f3fb260-f865-41af-aa12-8cdb0c12836f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0-10%      18864\n",
       "10-20%     10085\n",
       "20-30%      3773\n",
       "30-40%      2226\n",
       "40-50%      1849\n",
       "80-90%      1759\n",
       "50-60%      1078\n",
       "60-70%       733\n",
       "70-80%       682\n",
       "90-100%       32\n",
       "Name: stratum, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_non_train['stratum'] = pd.cut(df_non_train['prob'], bins=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], \n",
    "#                        labels=['0-10%', '10-20%', '20-30%', '30-40%', '40-50%', '50-60%', '60-70%', '70-80%', '80-90%', '90-100%'], \n",
    "#                        include_lowest=True)\n",
    "\n",
    "# df_non_train['stratum'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c843565c-6a2c-4072-b0af-146fe6f28311",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **3.3 Annotate 10 random samples from each stratum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0efbca-11d8-4283-a8bf-34be50586b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate_df = df_non_train.groupby('stratum').apply(lambda x: x.sample(n=10, random_state=1)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c30050b-fa75-4bf7-af30-3c8dba61161b",
   "metadata": {},
   "source": [
    "#### **- If annotate off-line:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a264eb-b71b-4b64-b471-ee39166a0cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate_df.to_csv('miniLM_annotate.csv')\n",
    "\n",
    "# # Annotate on 'miniLM_annotate.csv'\n",
    "# # Update the annotated file into miniLM_annotated.csv \n",
    "# annotate_df = pd.read_csv('miniLM_annotated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9273e6c-4301-4b2c-b0e4-1b1842440e85",
   "metadata": {},
   "source": [
    "#### **- If annotate on Jupyter Lab/Notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3f1e9e-c1d7-4d73-a856-37a220431528",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \n",
    "print(annotate_df['raw_X'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21feda28-85f1-4d2b-8293-effc1ecbc400",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate_df[index, 'real_y'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d5d07-cdcb-4330-a99d-af29c27acbed",
   "metadata": {},
   "source": [
    "#### **3.4 Calculate F-1 for each cutoff (9 in total) and find the best cutoff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267adee2-2d51-40f4-aacf-b3d59604ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "cutoffs = [i/10 for i in range(1, 10)]\n",
    "best_cutoff = None\n",
    "best_f1_score = -1\n",
    "\n",
    "for cutoff in cutoffs:\n",
    "    label_pred = [1 if p >= cutoff else 0 for p in annotated_df['prob']]\n",
    "    f1 = f1_score(annotated_df['real_y'], preds)\n",
    "    if f1 > best_f1_score:\n",
    "        best_cutoff = cutoff\n",
    "        best_f1_score = f1\n",
    "\n",
    "print(f'The optimal cutoff is {best_cutoff} which gives an F1 Score of {best_f1_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bae5f2-1b2c-47a7-a9e1-0727fde7644b",
   "metadata": {},
   "source": [
    "#### **3.5 Compare inter-model performance and finalize model choice**\n",
    "- miniLM (this file)\n",
    "- logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313c4a56-7b67-45b3-9b83-4166f6935338",
   "metadata": {},
   "source": [
    "#### Note: Helpful Code snippets\n",
    "\n",
    "1. Put mistranscribed transcripts into a dataframe\n",
    "```\n",
    "indices = df_before_cp[df_before_cp['is_mistranscribed']].index.tolist()\n",
    "paths = df_before_cp[df_before_cp['is_mistranscribed']]['path'].tolist()\n",
    "labels = df_before_cp[df_before_cp['is_mistranscribed']]['label'].tolist()\n",
    "texts = df_before_cp[df_before_cp['is_mistranscribed']]['content'].tolist()\n",
    "df_mistranscribed = pd.DataFrame({\"index\":indices, \"path\":paths, \"label\":labels, \"text\":texts})\n",
    "```\n",
    "\n",
    "2. Find non-English text inside dataframe df_before_cp \n",
    "```\n",
    "non_english_entries = df_before_cp.loc[~df_before_cp['is_English']]\n",
    "non_english_entries['text']\n",
    "```\n",
    "\n",
    "3. Find the web URL of mistranscribed transcripts, based on its file path\n",
    "```\n",
    "match = df_before_labeled[df_before_labeled['path'].str.contains('/shared/3/projects/benlitterer/podcastData/prosodyMerged/beforeFMonth/sphinx.acast.com/am/httpssphinx.acast.commorgenbladetyahyahassanmedia.mp3MERGED')]\n",
    "match['enclosure']\n",
    "```\n",
    "\n",
    "4. Count mistranscribed entries:\n",
    "```\n",
    "counts = (df[\"is_mistranscribed\"]).sum()\n",
    "```\n",
    "\n",
    "5. Find the entry and indices under one column with the longest/shortest length\n",
    "```\n",
    "max_length = df_before_cp['text'].apply(len).max()\n",
    "print(max_length)\n",
    "index_of_max_length = df_before_cp['text'].apply(len).idxmax()\n",
    "```\n",
    "\n",
    "6. Find the number of cells under column \"text\" that contain fewer than 100 words:\n",
    "```\n",
    "df_before['content'].apply(lambda x: len(str(x).split()) < 100).sum()\n",
    "```\n",
    "Similarly, grab those entries:\n",
    "```\n",
    "filtered_df = df_before[df_before['content'].apply(lambda x: len(str(x).split()) < 128)]\n",
    "```\n",
    "\n",
    "7. Average number of lines in the directory\n",
    "```\n",
    "import os\n",
    "from pathlib import Path\n",
    "parent_dir_path = \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/beforeFMonth/\"\n",
    "file_count = 0\n",
    "line_count = 0\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(parent_dir_path):\n",
    "    for file in filenames:\n",
    "        if file.endswith('.mp3MERGED'):\n",
    "            file_path = Path(dirpath) / file\n",
    "            with open(file_path, 'r') as f:\n",
    "                file_line_count = sum(1 for line in f)\n",
    "                if file_line_count > 0: \n",
    "                    file_count += 1\n",
    "                    line_count += file_line_count\n",
    "\n",
    "average_lines = line_count / file_count if file_count != 0 else 0\n",
    "print(\"Average number of lines across all .txt files is: \", average_lines)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc2f4b7-c3c9-4f03-96be-305cbc295f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"/shared/3/projects/benlitterer/podcastData/prosodyMerged/beforeFMonth/chtbl.com/c3/httpschtbl.comtrack5899Epodtrac.comptsredirect.mp3traffic.omny.fmdclipse73c998e6e60432f8610ae210140c5b1dd54ace2063f43cf8e45ae3900375bf011b79adba22f453fb4ccae39013d3cffaudio.mp3utm_sourcePodcastin_playlistc5eecdf1cdcf4e91b49aae3900375bfeMERGED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "83f08187-8fe3-410e-92a7-ecae6fbf2363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'rssUrl', 'epTitle', 'epDescription', 'duration',\n",
       "       'pubDate', 'copyright', 'itunes:type', 'itunes:complete', 'guid',\n",
       "       'itunes:explicit', 'enclosure', 'itunes:image', 'transDict', 'id',\n",
       "       'title', 'lastUpdate', 'link', 'lastHttpStatus', 'dead', 'contentType',\n",
       "       'itunesId', 'originalUrl', 'itunesAuthor', 'itunesOwnerName',\n",
       "       'explicit', 'imageUrl', 'itunesType', 'generator', 'newestItemPubdate',\n",
       "       'language', 'oldestItemPubdate', 'episodeCount', 'popularityScore',\n",
       "       'priority', 'createdOn', 'updateFrequency', 'chash', 'host',\n",
       "       'newestEnclosureUrl', 'podcastGuid', 'podDescription', 'category1',\n",
       "       'category2', 'category3', 'category4', 'category5', 'category6',\n",
       "       'category7', 'category8', 'category9', 'category10',\n",
       "       'newestEnclosureDuration', 'oldestItemDatetime', 'cleanDates', 'path',\n",
       "       'cleanDatesLoc', 'is_news'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_before_labeled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f12f804f-e485-4774-8366-ae0742a5a83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13    https://anchor.fm/s/8dc3d88/podcast/play/13515377/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fproduction%2F2020-4-9%2F71628068-44100-1-6102979586261.m4a\n",
       "Name: enclosure, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match = df_before[df_before['potentialOutPath'].str.contains(\"/shared/3/projects/benlitterer/podcastData/prosodyMerged/5_4_5_10/anchor.fm/2t/httpsanchor.fms8dc3d88podcastplay13515377https3A2F2Fd3ctxlq1ktw2nl.cloudfront.net2Fproduction2F2020492F716280684410016102979586261.m4aMERGED\")]\n",
    "match['enclosure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "601369d2-122d-434c-b3be-0b9134931391",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = df_before[df_before['is_news'] == 1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ae706730-6702-4245-9bd0-b99ec0e9ed62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16180, 3)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "077aac54-c6f3-449d-be11-5c62e454d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_before[df_before['content'].apply(lambda x: len(str(x).split()) > 120 and len(str(x).split()) < 400)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "42f0df35-3f01-4ad3-a567-c997ec37377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_rows = filtered_df[filtered_df['is_news'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "98805e65-4f07-41c8-94d5-6c69a78c279f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171495, 3)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_before.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "89a24595-6013-43d4-a541-fdb5c180c23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11238, 3)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b463440e-d24a-4d39-aacd-ff85ee6531cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
