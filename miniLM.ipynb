{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118572f8-4912-4eac-b812-4f6762dff0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in /opt/anaconda/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/anaconda/lib/python3.9/site-packages (from sentence-transformers) (4.35.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda/lib/python3.9/site-packages (from sentence-transformers) (4.64.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/anaconda/lib/python3.9/site-packages (from sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda/lib/python3.9/site-packages (from sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda/lib/python3.9/site-packages (from sentence-transformers) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda/lib/python3.9/site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda/lib/python3.9/site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: nltk in /opt/anaconda/lib/python3.9/site-packages (from sentence-transformers) (3.7)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda/lib/python3.9/site-packages (from sentence-transformers) (0.1.96)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/anaconda/lib/python3.9/site-packages (from sentence-transformers) (0.20.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.5.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: sympy in /opt/anaconda/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (2.11.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/anaconda/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/anaconda/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/anaconda/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/anaconda/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/anaconda/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/anaconda/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/anaconda/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/anaconda/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/anaconda/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/anaconda/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/anaconda/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/anaconda/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers) (67.6.0)\n",
      "Requirement already satisfied: wheel in /opt/anaconda/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers) (0.40.0)\n",
      "Requirement already satisfied: cmake in /opt/anaconda/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.26.3)\n",
      "Requirement already satisfied: lit in /opt/anaconda/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.3.15)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /opt/anaconda/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/anaconda/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.1)\n",
      "Requirement already satisfied: click in /opt/anaconda/lib/python3.9/site-packages (from nltk->sentence-transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in /opt/anaconda/lib/python3.9/site-packages (from nltk->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda/lib/python3.9/site-packages (from torchvision->sentence-transformers) (9.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/anaconda/lib/python3.9/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
      "Collecting huggingface-hub>=0.4.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda/lib/python3.9/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.6.15)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda/lib/python3.9/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.2.1)\n",
      "Installing collected packages: huggingface-hub\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 2.16.1 requires huggingface-hub>=0.19.4, but you have huggingface-hub 0.17.3 which is incompatible.\n",
      "flair 0.11.3 requires sentencepiece==0.1.95, but you have sentencepiece 0.1.96 which is incompatible.\n",
      "textattack 0.3.8 requires datasets==2.4.0, but you have datasets 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-0.17.3\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m[2024-01-21 19:34:27,921] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.special import softmax\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6ed85a-7450-4c39-9592-13f751b09d9a",
   "metadata": {},
   "source": [
    "### **1. Obtain the training and non-training data**\n",
    "- **Read in df_train.csv and df_non_train.csv**\n",
    "- df_train: dataframe for the training data\n",
    "- df_non_train: dataframe for the non-training data\n",
    "- columns=['X', 'raw_X', 'y', 'real_y', 'prob', 'path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f53f807d-08c4-488c-93e5-67291634db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('df_train.csv')\n",
    "df_non_train = pd.read_csv('df_non_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02505d4d-c60f-4baa-b043-dfaec03920fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.rename(columns={'y':'label', 'raw_X':'text'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145144a4-0418-4684-8288-a801bf62cfbd",
   "metadata": {},
   "source": [
    "### **2. Model training**\n",
    "- MiniLM (microsoft/MiniLM-L12-H384-uncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26e715ae-e1e6-4815-bde4-ac22170246ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_miniLM = df_train[['label', 'text']].copy()\n",
    "ds_train_miniLM = Dataset.from_pandas(df_train_miniLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2941d9e-011a-4937-8349-1350418eb3d4",
   "metadata": {},
   "source": [
    "### **Note: May change the following parameters as per David's advice:**\n",
    "- max_length=512 in tokenizer()\n",
    "- per_device_train_batch_size=64 in TrainingArguments()\n",
    "- num_train_epochs=3 in TrainingArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2914196-554d-411e-ab49-e3699ec6e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/MiniLM-L12-H384-uncased\")\n",
    "miniLM = AutoModelForSequenceClassification.from_pretrained(\"microsoft/MiniLM-L12-H384-uncased\", num_labels=2)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "tokenized_datasets = ds_train_miniLM.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "908ecb71-5044-45f5-9c18-fc2041cef070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e762124ba8435ca1f9327666be39ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39694 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:32: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 2 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/anaconda/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/anaconda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/anaconda/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 1564, in forward\n    outputs = self.bert(\n  File \"/opt/anaconda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/anaconda/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 1013, in forward\n    encoder_outputs = self.encoder(\n  File \"/opt/anaconda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/anaconda/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 607, in forward\n    layer_outputs = layer_module(\n  File \"/opt/anaconda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/anaconda/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 497, in forward\n    self_attention_outputs = self.attention(\n  File \"/opt/anaconda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/anaconda/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 427, in forward\n    self_outputs = self.self(\n  File \"/opt/anaconda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/anaconda/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 349, in forward\n    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 768.00 MiB (GPU 0; 47.54 GiB total capacity; 23.38 GiB already allocated; 144.94 MiB free; 23.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 14\u001b[0m\n\u001b[1;32m      8\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39mminiLM,\n\u001b[1;32m     11\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     12\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_datasets\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.9/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.9/site-packages/transformers/trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1860\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1865\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.9/site-packages/transformers/trainer.py:2725\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2722\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2724\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2725\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2728\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.9/site-packages/transformers/trainer.py:2748\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2747\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2748\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2749\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2750\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    170\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 171\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:181\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py:89\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     87\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m---> 89\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.9/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/anaconda/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/anaconda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/anaconda/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 1564, in forward\n    outputs = self.bert(\n  File \"/opt/anaconda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/anaconda/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 1013, in forward\n    encoder_outputs = self.encoder(\n  File \"/opt/anaconda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/anaconda/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 607, in forward\n    layer_outputs = layer_module(\n  File \"/opt/anaconda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/anaconda/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 497, in forward\n    self_attention_outputs = self.attention(\n  File \"/opt/anaconda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/anaconda/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 427, in forward\n    self_outputs = self.self(\n  File \"/opt/anaconda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/anaconda/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 349, in forward\n    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 768.00 MiB (GPU 0; 47.54 GiB total capacity; 23.38 GiB already allocated; 144.94 MiB free; 23.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "# Training miniLM on tokenized text\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", per_device_train_batch_size=64, num_train_epochs=3)\n",
    "trainer = Trainer(\n",
    "    model=miniLM,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89806ea8-c8f6-4312-80ba-2ad316672b7b",
   "metadata": {},
   "source": [
    "### **3. Model calibration on non-train data**\n",
    "- Predict probabilities on each row & fill up \"prob\" column\n",
    "- Build strata profile\n",
    "- Annotate 10 random samples from each stratum\n",
    "- Calculate F-1 for each cutoff (9 in total)\n",
    "- Find the best cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1460b294-8e2f-459c-8ac9-8a4b1bf67bcf",
   "metadata": {},
   "source": [
    "#### **3.1 Predict probabilities & fill up the \"prob\" column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6b72ce-d40a-430d-ad7e-004c4bdf579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_inputs = df_non_train['raw_X'].tolist()\n",
    "predict_dset = Dataset.from_pandas(pd.DataFrame({\"text\": prediction_inputs}))\n",
    "\n",
    "tokenized_predict_dset = predict_dset.map(tokenize_function, batched=True)\n",
    "predictions = trainer.predict(tokenized_predict_dset)\n",
    "\n",
    "# Apply softmax to the model's raw predictions to get probabilities\n",
    "probs = softmax(predictions.predictions, axis=1)[:, 1]\n",
    "\n",
    "df_non_train.loc[:, 'prob'] = probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1268dd-ce6c-4496-9bb8-ff7889e7c3b2",
   "metadata": {},
   "source": [
    "#### **3.2 Build strata profile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4f54ad-1955-44b2-a07a-2b50f531a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_train['stratum'] = pd.cut(df_non_train['prob'], bins=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], \n",
    "                       labels=['0-10%', '10-20%', '20-30%', '30-40%', '40-50%', '50-60%', '60-70%', '70-80%', '80-90%', '90-100%'], \n",
    "                       include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e350421b-98a4-400b-9f8e-7468255d588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution across different strata\n",
    "df_non_train['stratum'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d62af-3366-4cbb-bd1d-fbfddf45d94a",
   "metadata": {},
   "source": [
    "#### Original strata distribution:\n",
    "- 0-10%      20534  \n",
    "- 10-20%      9102\n",
    "- 20-30%      3375\n",
    "- 30-40%      2170\n",
    "- 40-50%      1757\n",
    "- 80-90%      1688\n",
    "- 50-60%      1022\n",
    "- 60-70%       734\n",
    "- 70-80%       699\n",
    "- 90-100%        0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b960d9d3-7830-4015-9c8f-26a2fb731432",
   "metadata": {},
   "source": [
    "#### **Note: Since 90-100% stratum is empty, I will increase all probabilities by 1.15% (Ask David or Ben)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a2510694-6bf1-4e74-b59c-6db8ef7bdd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_train['prob'] += 0.0115 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2f3fb260-f865-41af-aa12-8cdb0c12836f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0-10%      18864\n",
       "10-20%     10085\n",
       "20-30%      3773\n",
       "30-40%      2226\n",
       "40-50%      1849\n",
       "80-90%      1759\n",
       "50-60%      1078\n",
       "60-70%       733\n",
       "70-80%       682\n",
       "90-100%       32\n",
       "Name: stratum, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_non_train['stratum'] = pd.cut(df_non_train['prob'], bins=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], \n",
    "                       labels=['0-10%', '10-20%', '20-30%', '30-40%', '40-50%', '50-60%', '60-70%', '70-80%', '80-90%', '90-100%'], \n",
    "                       include_lowest=True)\n",
    "\n",
    "df_non_train['stratum'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c843565c-6a2c-4072-b0af-146fe6f28311",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **3.3 Annotate 10 random samples from each stratum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0efbca-11d8-4283-a8bf-34be50586b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate_df = df_non_train.groupby('stratum').apply(lambda x: x.sample(n=10, random_state=1)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c30050b-fa75-4bf7-af30-3c8dba61161b",
   "metadata": {},
   "source": [
    "#### **- If annotate off-line:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a264eb-b71b-4b64-b471-ee39166a0cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate_df.to_csv('miniLM_annotate.csv')\n",
    "\n",
    "# Annotate on 'miniLM_annotate.csv'\n",
    "# Update the annotated file into miniLM_annotated.csv \n",
    "annotate_df = pd.read_csv('miniLM_annotated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9273e6c-4301-4b2c-b0e4-1b1842440e85",
   "metadata": {},
   "source": [
    "#### **- If annotate on Jupyter Lab/Notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3f1e9e-c1d7-4d73-a856-37a220431528",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \n",
    "print(annotate_df['raw_X'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21feda28-85f1-4d2b-8293-effc1ecbc400",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate_df[index, 'real_y'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d5d07-cdcb-4330-a99d-af29c27acbed",
   "metadata": {},
   "source": [
    "#### **3.4 Calculate F-1 for each cutoff (9 in total) and find the best cutoff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267adee2-2d51-40f4-aacf-b3d59604ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "cutoffs = [i/10 for i in range(1, 10)]\n",
    "best_cutoff = None\n",
    "best_f1_score = -1\n",
    "\n",
    "for cutoff in cutoffs:\n",
    "    preds = [1 if p >= cutoff else 0 for p in annotated_df['prob']]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bae5f2-1b2c-47a7-a9e1-0727fde7644b",
   "metadata": {},
   "source": [
    "#### **3.5 Compare inter-model performance and finalize model choice**\n",
    "- miniLM (this file)\n",
    "- logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1000b6f2-f6af-4fbd-8a39-02153fc15e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "046eb878e2624071ad681c76ddbfe8be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1c1a0ab561004026b13ddad11e0dae22": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2a50f9f1a989468a80d86360cfe79254": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2fd5eac80feb4a0fbd1ef29478781fd9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "304f1981ab814880bd25ab02eaa954b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_bf690eb1df2c4a178a265859eeb85c1b",
       "max": 1,
       "style": "IPY_MODEL_b4ff5787dc914b69940e66e17855801d"
      }
     },
     "325741e77ed946188bb08282fb36e658": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_69a303c9c05f4eb0bc8ad80339552afb",
       "style": "IPY_MODEL_ab6f265fc3c042f4a6a4583fe67ea833",
       "value": " 39694/39694 [00:20&lt;00:00, 2114.12 examples/s]"
      }
     },
     "35075c56c235422c8c7dafea9d4bd6a3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "37110d69884641859df1fe48d0288e2f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "415f61e2d30d49e386ef70522674aa19": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_f974675827714d34a406a6b7584b2580",
        "IPY_MODEL_9af066fa7e6d4056a14a094c3a27e28c",
        "IPY_MODEL_572f00d458d1401c86706f8122c58845"
       ],
       "layout": "IPY_MODEL_7549f7c4205443cf946c898138752248"
      }
     },
     "50be279bd0b840179d663bb735cb3934": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "572f00d458d1401c86706f8122c58845": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_643d66d54b224d12a006865307792b1b",
       "style": "IPY_MODEL_5c9ff511547641fc90eb7a48f2940e61",
       "value": " 41080/41080 [00:22&lt;00:00, 1613.57 examples/s]"
      }
     },
     "577911716ac348008bd68665e2e3ac4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "5c9ff511547641fc90eb7a48f2940e61": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "5dea1cd3f24a48caa6db1a2df6f64939": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5debfd555a9f4f3f860bdc4a06ba4608": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_dce79389789d4ce69f90a9142224789c",
       "max": 39694,
       "style": "IPY_MODEL_577911716ac348008bd68665e2e3ac4d",
       "value": 39694
      }
     },
     "643d66d54b224d12a006865307792b1b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "655868d8e73640ba994e5feb48f3023f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "68fac31296054099ad4e02c22586aedd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_37110d69884641859df1fe48d0288e2f",
       "max": 1,
       "style": "IPY_MODEL_046eb878e2624071ad681c76ddbfe8be"
      }
     },
     "69a303c9c05f4eb0bc8ad80339552afb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7549f7c4205443cf946c898138752248": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8f96a5a1e52f4155959296c0b138a283": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "92c50d5433764f2dbd9e1a7cb4bed10b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_8f96a5a1e52f4155959296c0b138a283",
       "style": "IPY_MODEL_2a50f9f1a989468a80d86360cfe79254"
      }
     },
     "94e762124ba8435ca1f9327666be39ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c7cd80a87c4542f58ff8e1a0ed6dc3af",
        "IPY_MODEL_5debfd555a9f4f3f860bdc4a06ba4608",
        "IPY_MODEL_325741e77ed946188bb08282fb36e658"
       ],
       "layout": "IPY_MODEL_98a703f852e54605a308faf79882277e"
      }
     },
     "96f61af653d8422fb4b0caf9959ad1fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "98a703f852e54605a308faf79882277e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9af066fa7e6d4056a14a094c3a27e28c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_35075c56c235422c8c7dafea9d4bd6a3",
       "max": 41080,
       "style": "IPY_MODEL_d0022b1185d045619e70954a5d7128ff",
       "value": 41080
      }
     },
     "ab6f265fc3c042f4a6a4583fe67ea833": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b2685495d6f44eae8e61376c5ba9f8f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_2fd5eac80feb4a0fbd1ef29478781fd9",
       "style": "IPY_MODEL_96f61af653d8422fb4b0caf9959ad1fc"
      }
     },
     "b4ff5787dc914b69940e66e17855801d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "be248829fac84a27bcb97a9a9d63a9ad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bf4a2c348a1a47aa92621bd1d2d0cc8d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_92c50d5433764f2dbd9e1a7cb4bed10b",
        "IPY_MODEL_304f1981ab814880bd25ab02eaa954b4"
       ],
       "layout": "IPY_MODEL_f7d8a89698a64e0ca102a5fffe41b37c"
      }
     },
     "bf690eb1df2c4a178a265859eeb85c1b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c7cd80a87c4542f58ff8e1a0ed6dc3af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_be248829fac84a27bcb97a9a9d63a9ad",
       "style": "IPY_MODEL_655868d8e73640ba994e5feb48f3023f",
       "value": "Map: 100%"
      }
     },
     "d0022b1185d045619e70954a5d7128ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "dce79389789d4ce69f90a9142224789c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "eb6374f70b6844c3b3aeba8afb007b5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_b2685495d6f44eae8e61376c5ba9f8f3",
        "IPY_MODEL_68fac31296054099ad4e02c22586aedd"
       ],
       "layout": "IPY_MODEL_5dea1cd3f24a48caa6db1a2df6f64939"
      }
     },
     "f7d8a89698a64e0ca102a5fffe41b37c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f974675827714d34a406a6b7584b2580": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_50be279bd0b840179d663bb735cb3934",
       "style": "IPY_MODEL_1c1a0ab561004026b13ddad11e0dae22",
       "value": "Map: 100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
